{
  "hash": "765c62ab2f76fd8b23c1308ca6ab2fdf",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat:\n  revealjs:\n    height: 900\n    width: 1600\n    logo: \"images/osu.png\"\n    slide-number: true\n    scrollable: true\n---\n\n## Copula-Based Mixture Transition Distribution Models for Forecasting Skewed and Zero-Inflated Time Series: Methodology and Comparisons with Deep Learning LSTM Networks  \n\n<br>\n\n<br>\n\n<br>\n\n::::: columns\n\n::: column\n\n### Frances Lin\n\nüõ° Dissertation Defense  \n\nü¶´ Oregon State University  \n\nNovember 5, 2025\n\n::: \n\n::: column\n\n###\n\nCo-Advisors: Dr. Lisa Madsen, Dr. Charlotte Wickham\n\nCommittee Members: Dr. James Molyneux, Dr. Claudio Fuentes, Dr. Prasad Tadepalli\n\n::: \n\n::::: \n\n## Motivation  \n\nModeling complex patterns in sequence data is a central task across domains such as energy, insurance, and transportation. \n\n- ‚ö†Ô∏è Real-world time series often show skewness and zero inflation, which can undermine performance if not properly addressed. \n\n. . .    \n\nRecent AI advances, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, effectively capture nonlinear and long-range dependence. \n\n- ‚ö†Ô∏è Claims of LSTM superiority can be misleading when compared to inappropriate benchmarks. \n\n- ‚ö†Ô∏è A prior work found comparable performance between probabilistic Mixture Transition Distribution (MTD) model and deep learning LSTM for predicting, e.g., disease spread. \n\n--- \n\n![](images/nasa-1.jpg){.absolute top=0 left=0 width=\"900\" height=\"675\"}\n\n![](images/nasa-2.jpg){.absolute top=0 left=950 width=\"250\" height=\"675\"}\n\n![](images/nasa-3.jpg){.absolute top=0 left=1250 width=\"250\" height=\"675\"}\n\n<div style=\"margin-top: 725px;\">\n¬©Ô∏è NASA GES DISC\nüìç Limon Wind Energy Center, Colorado  \n</div>\n\n## NASA MERRA-2 Data Hourly Wind Speeds, Key Proxy for Energy Forecasting \n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### windspeeds at 50m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 10m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 2m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n::: \n\n## Objectives \n\nPart I and II of the dissertation\n\n> Develop the copula-based Gamma Mixture Transition Distribution (Gamma MTD) model and its zero-inflated extension (ZIGamma MTD) to capture high-order dependence, skewed behavior, and semicontinuous patterns.\n\n. . .\n\nPart III of the dissertation\n\n> Evaluate the proposed models alongside deep learning approaches, specifically the Long Short-Term Memory (LSTM) networks, demonstrating higher predictive accuracy and robustness.\n\n## Roadmap \n\nPart I Models for Forecasting Skewed Time Series \n\n- Recap of MTD Models \n\n- What's New \n\nPart II Models for Forecasting Zero-Inflated Skewed Time Series \n\n- Introduction \n\n- Proposed Method \n\n- Simulation Studies \n\n- Prediction \n\n## Roadmap \n\nPart III Copula-Based Probabilistic MTD Models vs. Deep Learning LSTM Networks\n\n- Introduction \n\n- Training, Hyperparameter Tuning, and Metrics \n\n- (Skip) Simulation Studies \n\n- Data Application \n\n# Part I Models for Forecasting Skewed Time Series {background-color=\"#D73F09\" color=\"white\"} \n\n## Introduction \n\nTime series model captures how past values contribute to current value and uses this information to predict future value.\n\n. . .    \n\nThe mixture transition distribution (MTD) models \n\n- Extend the autoregressive (AR) models to accommodate non-Gaussian and nonlinear features. \n\n- Model high-order dependence as a mixture of first-order transition kernels, with random, stochastic weights. \n\n- See Berchtold and Raftery (2022) for a complete review. \n\n## Introduction and Objectives \n\nOur work builds upon the architecture of the MTD model introduced by Zheng et al. (2022). \n\n- Includes various applications, such as Gaussian, Poisson, negative binomial, and Lomax regression MTD models, extending beyond linear, Gaussian dynamics. \n\nHowever, for certain invariant marginal distributions, the transition kernel may either require careful construction or can result in a form that is not explicitly defined or too complex. \n\n. . .\n\n> **Objectives:** Propose to incorporate copulas into the transition kernels to address this limitation, thereby enhancing modeling capabilities and flexibility.\n\n. . .\n\n> Develop the Gamma MTD model, but the framework is general.\n\n## Model Framework: DAG; also known as Bayesian Network\n\nLet $\\{ X_t: t \\in \\mathbb{N} \\}$ be a time series. Construct the model on a DAG, the joint distribution of $X_1$ to $X_t$ can be factorized into a product of conditional distributions as\n```{=latex}\n\\begin{equation}\nf(\\textbf{x}) = f(x_1) \\prod_{t=2}^t f(x_t | \\textbf{x}^{t-1}). \\tag{1}\n\\label{eq:joint_distribution}\n\\end{equation}\n```\n[$f(x_t | \\textbf{x}^{t-1})$ is the conditional probability density function (pdf) of current value $X_t$ given all of its past values $\\textbf{X}^{t-1} = \\textbf{x}^{t-1}$, where $\\textbf{X}^{t-1} = \\{X_i: i \\leq t - 1 \\}$ and $\\textbf{x}^{t-1} = \\{x_i: i \\leq t - 1 \\}$.]{style=\"font-size: 80%\"}\n\n. . .\n\nAs $t$ increases, the size of the conditioning set, $\\textbf{X}^{t-1}$ (all of its past values), can be large. \n\n- Solution: Use a structured mixture model that restricts each value in the MTD model to depend only on the $L$ (a given number) lagged values.\n\n. . .\n\nEach conditional in (1) is modeled as a mixture of transition kernels, with weight assigned to each of the $L$ lagged values.\n\n## MTD Order (Look-Back Steps) $L = 5$\n\n```{dot}\ngraph {\n  rankdir=LR; // left-to-right layout\n  node [shape=circle, style=solid, fontsize=10, fixedsize=true, width=0.6, fontname=\"Times-Roman\"];\n\n  // Define nodes\n  X1 [label=< <b>X</b><sub><b>t-5</b></sub> >];\n  X2 [label=< <b>X</b><sub><b>t-4</b></sub> >];\n  X3 [label=< <b>X</b><sub><b>t-3</b></sub> >];\n  X4 [label=< <b>X</b><sub><b>t-2</b></sub> >];\n  X5 [label=< <b>X</b><sub><b>t-1</b></sub> >];\n  X6 [label=< <b>X</b><sub><b>t</b></sub> >];\n\n  // Chain up to X5\n  X1 -- X2 -- X3 -- X4 -- X5;\n\n  { rank=same; X5; X6; }\n}\n```\n\n\n## Model Framework: Transition Kernels and Mixture Weights \n\nFor $t > L$, the MTD model specifies the conditional distribution of $X_t$ given $\\textbf{X}^{t-1} = \\textbf{x}^{t-1}$ as \n```{=latex}\n\\begin{equation}\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^L w_l f_l (x_t | x_{t-l}). \\tag{2}\n\\label{eq:cond_distribution}\n\\end{equation}\n```\n[$f_l (x_t | x_{t-l})$ is the conditional pdf of $X_t$ with respect to the $l$th transition kernel given that ${X}_{t-l} = {x}_{t-l}$. $w_l$ are weight parameters, where $w_l \\geq 0$ such that $\\sum_{l=1}^L w_l = 1$.]{style=\"font-size: 80%\"}\n\n. . .\n\nThere are two components in (2):\n\n  - Transition kernel, $f_l$, captures the influence of the $l$th lag value on the current value.  \n  \n  - Weight parameter, $w_l$, determines the relative contribution of that influence. \n\n--- \n\n![](images/financial.jpg){.absolute top=0 left=0 width=\"1200\" height=\"675\"}\n\n<div style=\"margin-top: 725px;\">\n¬©Ô∏è Yuvraj Deshmukh\n‚è≠Ô∏è Copula\n</div>\n\n## Copula \n\nCopula is widely used for dependence modeling.\n\n  - Is applied in quantitative finance, reliability engineering, hydrology, etc.\n\n  - Consists of families of copula, constituting a substantial research area.\n\n. . .\n\nCopula is a multivariate cumulative distribution function (cdf) where its marginal distribution of each random variables is $\\text{Unif}(0, 1)$. \n\n. . .\n\nUsing copula, any joint distribution, $F$, can be decomposed into two parts: the copula, $C$, and the marginal distributions, $F_j$, $j = 1,..., p$.\n\n- Rooted in Sklar‚Äôs theorem. [^sklar]\n\n[^sklar]: See Supplement. \n  \n\n## Existing Method to Proposed Method \n\nThe bivariate distribution approach, for example, identifies a bivariate distribution of $(U_l, V_l)$ such that the marginal densities, $f_{U_l}$ and $f_{V_l}$, are equal to a pre-specified stationary marginal density $f_X$ for all $l$ transition kernels. \n\n- Based on Proposition 1 [^prop1] of Zheng et al. (2022).\n\n. . .\n\nThis is facilitated by the use of a copula,  \n\n- which separates the marginal behavior of the random variables from their dependence structure. \n\n[^prop1]: See Supplement. \n\n## Proposed Method  \n\nReplace $f_{U_l}$ and $f_{V_l}$ with a pre-specified stationary marginal density, $f_X$, for every $x_t$ and for all $l$. For $t > L$, the proposed copula-based MTD model specifies the conditional distribution as\n```{=latex}\n\\begin{equation}\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^L w_l c_l (x_t, x_{t-l}) f_X(x_t). \\tag{3}\n\\label{eq:cond_distribution_copula}\n\\end{equation}\n```\n[$c_l (x_t, x_{t-l})$ is the copula density evaluated at $x_t$ and $x_{t-l}$, and $f_X(x_t)$ is the stationary marginal density evaluated at $x_t$.]{style=\"font-size: 80%\"}\n\n. . .\n\nCompared to (2), the transition kernel, $f_l$, is now replaced by two components: \n\n  - Copula density, $c_l$, captures and controls the strength of the dependence through a dependence parameter.\n  \n  - Stationary marginal density, $f_X$, describes the marginal behavior through distribution parameters.\n\n\n## Bayesian Implementation  \n\nEarlier MTD models were built through frequentist approaches. \n\nEstimation and prediction in the MTD model by Zheng et al. (2022) is constructed with Bayesian methodologies. \n\n- Our proposed copula-based MTD model builds upon this model version. \n  \n- We consider the Gaussian copula with Gamma marginals, i.e., Gamma MTD model, but the framework is generalizable, e.g., \n\n  - Lognormal MTD model or for any continuous distribution. \n\n  - Clayton copula or any other copula-based MTD model. \n\n## Bayesian Implementation  \n\nInference is facilitated through a set of latent variables ${\\{Z_t\\}}_{t=L+1}^n$. $Z_t = l$ indicates which $l$th kernel the data belongs to. \n\nThe posterior distribution of the parameters, based on the conditional likelihood, is\n```{=latex} \n\\begin{equation}\n\\begin{split}\np(\\boldsymbol{w}, \\boldsymbol{\\theta}, {\\{z_t\\}}_{t=L+1}^n | D_n) \\propto \\pi_w(\\boldsymbol{w}) \\prod_{l=1}^L \\pi_l(\\boldsymbol{\\theta}_l) \\\\ \n\\prod_{t=L+1}^n \\Biggl\\{ f_{z_t} (x_t | x_{t-z_t}, \\boldsymbol{\\theta}_{z_t}) \\sum_{l=1}^L w_l \\delta_l(z_t) \\Biggl\\}.\n\\end{split} \\tag{4}\n\\end{equation}\n```\n[$\\boldsymbol{w}$ is the mixture weights. $\\boldsymbol{\\theta}$ collects parameters for the transition kernel. $D_n = {\\{x_t\\}}_{t=L+1}^n$ is the data.]{style=\"font-size: 80%\"}\n\n. . .\n\nWith priors specifications [^prior], the full simulation-based Bayesian estimation and prediction can be achieved using Markov chain Monte Carlo (MCMC) algorithms. \n\n[^prior]: See Supplment. \n\n## Simulation Studies  \n\nWe have assessed accuracy and performance of the proposed model. \n\n- Convergence Diagnostics ($\\boldsymbol{w}$, $\\boldsymbol{\\rho}$, $\\alpha$, $\\beta$)\n\n- Weight and Dependence Parameters for Copula ($\\boldsymbol{w}$, $\\boldsymbol{\\rho}$)\n\n- Parameters for Marginal Distribution ($\\alpha$, $\\beta$)\n\n. . .\n\nSimulation studies demonstrate promising results across various scenarios. \n\n- Scenario 1 and 2: Default setup; Compatible $\\boldsymbol{w}$ and $\\boldsymbol{\\rho}$\n\n- Scenario 1.3 and 1.4: Incompatible $\\boldsymbol{w}$ and $\\boldsymbol{\\rho}$\n\n- Scenario 1.5 and 1.6: Compatible $\\boldsymbol{w}$ and $\\boldsymbol{\\rho}$; Rarely observable patterns\n\n- Scenario 3-6: Usual case for gamma marginals\n\n- Scenario 7-9: Unusual case involving high skewness\n\n## Weight and Dependence Parameters for Copula [^wrho]\n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n### Exponentially decreasing weights (s1)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n### Uneven arrangement of the relevant lags (s2)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n:::\n\n[^wrho]: Four other $\\boldsymbol{w}$ and $\\boldsymbol{\\rho}$ patterns (s1.3-1.6) are omitted here. \n\n## Parameters for Marginal Distribution\n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n### Usual Cases (s3-6)\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n### Unusual Cases (s7-9)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n::: \n\n## What's New  \n\nSince the preliminary exam, we have also conducted: \n\n- Sensitivity Analysis \n\n- Coverage Assessment \n\n- Prediction \n\nWe are not gonna discuss them here.\n\n# Part II Models for Forecasting Zero-Inflated Skewed Time Series {background-color=\"#D73F09\" color=\"white\"}\n\n## Introduction  \n\nZero-inflated data are characterized by an excess of zero values and  frequently appear in domains such as healthcare, insurance, environment, and transportation. \n\nExamples include \n\n- medical costs, \n\n- insurance claims, \n\n- precipitation amounts, and \n\n- safety measures.\n\nIf left unaddressed, zero-inflation undermines model robustness and weakens inference.\n\n## Introduction and Objectives \n\nIn Part I, we propose the copula-based Gamma MTD model, which enables flexible dependence modeling and accommodates arbitrary continuous marginals. \n\nHowever, \n\n- it remains limited in handling excessive zeros commonly observed in real-world continuous data. \n\n- Copula-based approaches face identifiability issues when modeling discrete or mixed marginals.\n\n. . .\n\n> **Objectives:** Propose reconstructing the marginal distribution to accommodate zero-inflation, while maintaining the same effectiveness and flexibility in modeling dependence structures as described in Part I.\n\n. . .\n\n> Develop the ZIGamma MTD model, but the framework is again general.\n\n## Existing Method to Proposed Method  \n\nOur approach is similar to hurdle models, one of the two primary frameworks for zero-inflated data, where zeros occur only in the binary part. \n\n. . .\n\nInstead of generating exact zeros, our approach applies a soft threshold that replaces them with small non-zero values.\n\n- Based on the continuous extension (CE) approach by Denuit and Lambert (2005). \n\n- Has only been applied in discrete and spatial settings, not in our setting. \n\n## Proposed Method  \n\nTo construct zero-inflated Gamma for the marginal distribution, \n\n- The Gamma distribution is first reparametrized in terms of the mean, $\\mu$, and the scale parameter, $\\beta$. Specifically,\n```{=latex} \n\\begin{equation}\nf(y; \\mu, \\beta) = \n\\frac{1} {\\Gamma(\\frac{\\mu}{\\beta}) \\beta^{\\frac{\\mu}{\\beta}}} {y}^{\\frac{\\mu}{\\beta} - 1} \\exp(- \\frac{y} {\\beta}) \\quad y \\geq 0, \\tag{5}\n\\end{equation}\n```\nwhere $\\frac{\\mu}{\\beta} > 0$ denotes the shape and $\\beta > 0$ the scale parameter.\n\n## Proposed Method  \n\n- Zero values are then replaced with non-zero values drawn from a uniform distribution. Specifically,\n```{=latex} \n\\begin{equation}\n0 \\leftarrow U_i, \\tag{6}\n\\end{equation}\n```\nwhere $U_i$ follows a continuous uniform distribution on $(0, \\epsilon)$ with $\\epsilon$ is a data-driven paramater representing the smallest observed non-zero values.\n\n## Proposed Method  \n\nThe resulting distribution, denoted as $\\text{ZIGamma}(\\mu, \\beta, P, \\epsilon)$, is expressed as:\n```{=latex} \n\\begin{equation}\nf(x) = \n\\begin{cases} \n\\text{Unif}(0, \\epsilon) & \\text{with probability } P \\\\\n\\text{ShiftedGamma}(\\mu, \\beta; \\epsilon) & \\text{with probability } 1-P, \n\\end{cases} \\tag{7}\n\\end{equation}\n```\n[where $\\mu$ denotes the mean and $\\beta$ the scale parameter of the shifted Gamma distribution, $P \\in [0, 1]$ the zero-inflated probability, and $\\epsilon > 0$ the threshold parameter.]{style=\"font-size: 80%\"} \n\n. . .\n\nIn (7), there are two parts: \n\n- We have already discussed the uniform distribution. \n\n- The shifted Gamma distribution [^shiftedgamma], $\\text{ShiftedGamma}(\\mu, \\beta; \\epsilon)$, is a standard Gamma distribution with mean $\\mu$ and scale $\\beta$ that is shifted to the right by $\\epsilon$, with the support $[\\epsilon, \\infty)$. \n\n[^shiftedgamma]: See Supplement. \n\n## dzig (pdf of zigamma)  \n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### $P = 0.1$ $\\epsilon = 0.1$\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.1$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.5$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.5$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.7$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.7$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## pzig (cdf of zigamma)  \n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### $P = 0.1$ $\\epsilon = 0.1$\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.1$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.5$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.5$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.7$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.7$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n:::\n\n--- \n\n![](images/hpc-3.jpg){.absolute top=0 left=0 width=\"1200\" height=\"675\"}\n\n<div style=\"margin-top: 725px;\">\n¬©Ô∏è FSP Group\nüôè OSU Advanced Research Computing Services, e.g., COSINE and Novus Clusters\n</div>\n\n## Simulation Setting   \n\nSimulated data: \n\n- With weight $\\boldsymbol{w}$, dependence for Gaussian copula $\\boldsymbol{\\rho}$, mean $\\mu$, scale $\\beta$, zero-inflated probability $P$, and threshold parameter $\\epsilon$, we generate $n = 2000$ observations from the copula-based ZIGamma MTD model.\n\nModel fitting: \n\n- Set the order $L = 5$ and consider the Gaussian copula with zero-inflated Gamma marginals.\n\nMCMC setting: \n\n- Run the Gibbs sampler for $165, 000$ iterations, discard the first $5000$ iterations as burn-in, and collect samples every $20$ iterations.\n\n- Run $4$ MCMC chains with $8000$ iterations each for all of the following scenarios. \n\n## Simulation Studies   \n\nWe have assessed accuracy and performance of the proposed model.\n\n- Convergence Diagnostics ($\\boldsymbol{w}$, $\\boldsymbol{\\rho}$, $\\mu$, $\\beta$, $P$, $\\epsilon$)\n\n- Weight and Dependence Parameters for Copula ($\\boldsymbol{w}$, $\\boldsymbol{\\rho}$)\n\n- Parameters for Marginal Distribution ($\\mu$, $\\beta$, $P$, $\\epsilon$)\n\nSimulation studies demonstrate promising results across various scenarios. \n\n- Scenario 1 and 2: Default setup; Compatible $\\boldsymbol{w}$ and $\\boldsymbol{\\rho}$\n\n- Scenario 3-6: Usual case for zero-inflated gamma marginals\n\n- Scenario 7-9: Unusual case involving high skewness\n\n## Simulation Studies   \n\nIn all nine scenarios (s1-9), we vary $P$ (zero-inflated probability) and $\\epsilon$ (threshold value), resulting in six cases per scenario: \n\n- $P = 0.1$, $\\epsilon = 0.1$\n\n- $P = 0.1$, $\\epsilon = 0.4$\n\n- $P = 0.5$, $\\epsilon = 0.1$\n\n- $P = 0.5$, $\\epsilon = 0.4$\n\n- $P = 0.7$, $\\epsilon = 0.1$\n\n- $P = 0.7$, $\\epsilon = 0.4$\n\nFor brevity, we present Scenario 1 only.\n\n## Simulation Results [^res] [^res2] \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: {.panel-tabset}\n\n### $P = 0.1$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.             |Mean (SD)       |R (Upper CI) | Naive SE| Time-series SE|\n|:-------------|:---------------|:------------|--------:|--------------:|\n|$w_1 = 0.636$ |0.6395 (0.0425) |1 (1)        |    2e-04|         0.0003|\n|$w_2 = 0.234$ |0.1905 (0.0636) |1.01 (1.01)  |    4e-04|         0.0013|\n|$w_3 = 0.086$ |0.1315 (0.0739) |1 (1)        |    4e-04|         0.0021|\n|$w_4 = 0.032$ |0.0346 (0.0529) |1.01 (1.03)  |    3e-04|         0.0017|\n|$w_5 = 0.012$ |0.0039 (0.0171) |1 (1)        |    1e-04|         0.0004|\n\n\n:::\n:::\n\n\n### $P = 0.1$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.                |Mean (SD)        |R (Upper CI) | Naive SE| Time-series SE|\n|:----------------|:----------------|:------------|--------:|--------------:|\n|$\\rho_1 = 0.700$ |0.6847 (0.0274)  |1 (1)        |   0.0002|         0.0002|\n|$\\rho_2 = 0.500$ |0.606 (0.1426)   |1.01 (1.01)  |   0.0008|         0.0027|\n|$\\rho_3 = 0.300$ |0.1168 (0.2389)  |1 (1)        |   0.0013|         0.0018|\n|$\\rho_4 = 0.100$ |0.0147 (0.4675)  |1 (1)        |   0.0026|         0.0027|\n|$\\rho_5 = 0.100$ |-0.0046 (0.5659) |1 (1)        |   0.0032|         0.0032|\n\n\n:::\n:::\n\n\n::: \n\n[^res]: There is no evidence of lack of convergence for all parameters (Gelman-Rubin statistic $R$  and its upper CI $\\leq$ $1.1$). \n\n[^res2]: $\\boldsymbol{w}$, $\\boldsymbol{\\rho}$ for all other combinations of $P = 0.1, 0.5, 0.7$ and $\\epsilon = 0.1, 0.4$ are similar and therefore omitted.\n\n## Simulation Results [^res]  \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: {.panel-tabset}\n\n### $P = 0.1$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.          |Mean (SD)       |R (Upper CI) | Naive SE| Time-series SE|\n|:----------|:---------------|:------------|--------:|--------------:|\n|$\\mu$      |7.35 (0.1132)   |1 (1)        |    6e-04|          6e-04|\n|$\\beta$    |1.0082 (0.0433) |1 (1)        |    2e-04|          2e-04|\n|$P$        |0.0769 (0.0085) |1 (1)        |    0e+00|          0e+00|\n|$\\epsilon$ |0.1 (7e-04)     |1 (1)        |    0e+00|          0e+00|\n\n\n:::\n:::\n\n\n### $P = 0.1$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.          |Mean (SD)       |R (Upper CI) | Naive SE| Time-series SE|\n|:----------|:---------------|:------------|--------:|--------------:|\n|$\\mu$      |7.1454 (0.12)   |1 (1)        |    7e-04|          7e-04|\n|$\\beta$    |1.0994 (0.0472) |1 (1)        |    3e-04|          3e-04|\n|$P$        |0.1091 (0.0103) |1 (1)        |    1e-04|          1e-04|\n|$\\epsilon$ |0.4017 (0.0019) |1 (1)        |    0e+00|          0e+00|\n\n\n:::\n:::\n\n\n### $P = 0.5$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.          |Mean (SD)       |R (Upper CI) | Naive SE| Time-series SE|\n|:----------|:---------------|:------------|--------:|--------------:|\n|$\\mu$      |6.9447 (0.1207) |1 (1)        |    7e-04|          7e-04|\n|$\\beta$    |1.0659 (0.0542) |1 (1)        |    3e-04|          3e-04|\n|$P$        |0.5248 (0.0172) |1 (1)        |    1e-04|          1e-04|\n|$\\epsilon$ |0.1001 (1e-04)  |1 (1)        |    0e+00|          0e+00|\n\n\n:::\n:::\n\n\n### $P = 0.5$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.          |Mean (SD)       |R (Upper CI) | Naive SE| Time-series SE|\n|:----------|:---------------|:------------|--------:|--------------:|\n|$\\mu$      |6.8454 (0.1154) |1 (1)        |    6e-04|          6e-04|\n|$\\beta$    |1.0086 (0.0512) |1 (1)        |    3e-04|          3e-04|\n|$P$        |0.5064 (0.0173) |1 (1)        |    1e-04|          1e-04|\n|$\\epsilon$ |0.4 (4e-04)     |1 (1)        |    0e+00|          0e+00|\n\n\n:::\n:::\n\n\n### $P = 0.7$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.          |Mean (SD)       |R (Upper CI) | Naive SE| Time-series SE|\n|:----------|:---------------|:------------|--------:|--------------:|\n|$\\mu$      |6.988 (0.1303)  |1 (1)        |    7e-04|          7e-04|\n|$\\beta$    |0.9593 (0.0594) |1 (1)        |    3e-04|          3e-04|\n|$P$        |0.6879 (0.016)  |1 (1)        |    1e-04|          1e-04|\n|$\\epsilon$ |0.0999 (1e-04)  |1 (1)        |    0e+00|          0e+00|\n\n\n:::\n:::\n\n\n### $P = 0.7$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.          |Mean (SD)       |R (Upper CI) | Naive SE| Time-series SE|\n|:----------|:---------------|:------------|--------:|--------------:|\n|$\\mu$      |6.8482 (0.1373) |1 (1)        |    8e-04|          8e-04|\n|$\\beta$    |1.0506 (0.0665) |1 (1)        |    4e-04|          4e-04|\n|$P$        |0.7048 (0.0154) |1 (1)        |    1e-04|          1e-04|\n|$\\epsilon$ |0.4002 (3e-04)  |1 (1)        |    0e+00|          0e+00|\n\n\n:::\n:::\n\n\n:::\n\n[^res]: There is no evidence of lack of convergence for all parameters (Gelman-Rubin statistic $R$  and its upper CI $\\leq$ $1.1$). \n\n## Coverage Assessment\n\nAll nine scenarios (s1-9) were analyzed using $4$ chains with only one replicate. \n\n- Recall that we have six cases per scenario.\n\n. . .\n\nScenarios 1 and 2 were further evaluated with $40$ replicates to assess coverage and robustness. \n\n- The overall coverage is the proportion of replicates for which the true value is contained within the interval. \n\n- Most parameters achieve full coverage, with a few slightly below $1$, indicating that the posterior intervals reliably capture the true parameter values. \n\n## Coverage Assessment for Scenario 1 and 2, 40 Replicates [^coveragezigamma] [^coveragezigamma2]\n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### Exponentially decreasing weights (s1)\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|         | $\\mu$| $\\beta$|   $P$| $\\epsilon$| $w_1$| $w_2$| $w_3$| $w_4$| $w_5$| $\\rho_1$| $\\rho_2$| $\\rho_3$| $\\rho_4$| $\\rho_5$|\n|:--------|-----:|-------:|-----:|----------:|-----:|-----:|-----:|-----:|-----:|--------:|--------:|--------:|--------:|--------:|\n|P01eps01 | 0.950|   0.925| 0.950|      0.925| 0.925| 0.925| 1.000| 1.000| 1.000|    0.875|    1.000|        1|    0.975|        1|\n|P01eps04 | 0.975|   0.975| 0.975|      0.925| 0.950| 0.950| 0.975| 1.000| 0.975|    0.975|    1.000|        1|    0.975|        1|\n|P05eps01 | 0.950|   0.950| 0.925|      0.975| 0.875| 0.950| 1.000| 1.000| 0.925|    0.925|    1.000|        1|    1.000|        1|\n|P05eps04 | 0.975|   0.925| 0.950|      1.000| 0.950| 1.000| 1.000| 1.000| 0.925|    0.875|    0.975|        1|    1.000|        1|\n|P07eps01 | 0.950|   0.950| 0.925|      0.975| 1.000| 1.000| 1.000| 1.000| 0.950|    0.925|    1.000|        1|    1.000|        1|\n|P07eps04 | 0.925|   1.000| 1.000|      0.925| 0.950| 0.925| 0.950| 0.975| 0.950|    1.000|    0.975|        1|    0.975|        1|\n\n\n:::\n:::\n\n\n[^coveragezigamma]: Each row label indicates the combination of $P$ (zero-inflated probability) and $\\epsilon$ (threshold value). For example, P01eps01 reads as $P = 0.1$ and $\\epsilon = 0.1$. \n\n[^coveragezigamma2]: Each row involves $40$ replicates, consisting of $4$ chains with $8,000$ samples per chain.\n\n### Uneven arrangement of the relevant lags (s2)\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|         | $\\mu$| $\\beta$|   $P$| $\\epsilon$| $w_1$| $w_2$| $w_3$| $w_4$| $w_5$| $\\rho_1$| $\\rho_2$| $\\rho_3$| $\\rho_4$| $\\rho_5$|\n|:--------|-----:|-------:|-----:|----------:|-----:|-----:|-----:|-----:|-----:|--------:|--------:|--------:|--------:|--------:|\n|P01eps01 | 0.925|   0.950| 0.925|      0.900|     1| 0.975| 0.975|     1| 0.975|    1.000|    0.975|    0.975|        1|    0.975|\n|P01eps04 | 0.875|   0.950| 0.950|      0.875|     1| 1.000| 0.975|     1| 0.925|    0.975|    0.950|    0.975|        1|    1.000|\n|P05eps01 | 0.925|   0.950| 0.950|      0.950|     1| 1.000| 0.975|     1| 0.950|    1.000|    0.975|    1.000|        1|    0.975|\n|P05eps04 | 0.950|   0.975| 0.875|      0.975|     1| 1.000| 0.925|     1| 0.975|    1.000|    1.000|    1.000|        1|    0.975|\n|P07eps01 | 0.950|   0.950| 0.900|      0.975|     1| 1.000| 0.975|     1| 0.875|    1.000|    0.950|    0.950|        1|    0.900|\n|P07eps04 | 0.975|   0.950| 0.975|      0.950|     1| 1.000| 1.000|     1| 0.975|    1.000|    0.975|    0.975|        1|    1.000|\n\n\n:::\n:::\n\n\n::: \n\n## Prediction, $95\\%$ Predictive Intervals, One-Step Ahead {#sec-ch2-prediction}\n\n::::: columns\n\n::: column\n\n### \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|         |Coverage |Below                                                |Above                                                |\n|:--------|:--------|:----------------------------------------------------|:----------------------------------------------------|\n|P01Eps01 |0.9549   |0.6944                                               |<span style=\" font-weight: bold;    \" >0.9751</span> |\n|P01Eps04 |0.9148   |0.3973                                               |<span style=\" font-weight: bold;    \" >0.9786</span> |\n|P05Eps01 |0.9278   |<span style=\" font-weight: bold;    \" >0.9876</span> |0.8615                                               |\n|P05Eps04 |0.9484   |<span style=\" font-weight: bold;    \" >0.9681</span> |0.9285                                               |\n|P07Eps01 |0.8677   |<span style=\" font-weight: bold;    \" >0.9943</span> |0.5726                                               |\n|P07Eps04 |0.9298   |<span style=\" font-weight: bold;    \" >0.9888</span> |0.7828                                               |\n\n\n:::\n:::\n\n\n::: \n\n::: column\n\n### \n\n::: {.incremental}\n\n- When the zero-inflated probability is low (e.g., $P = 0.1$), the empirical coverage **above** (i.e., the coverage for data greater than $\\epsilon$) is a more informative metric for assessing predictive performance. \n\n- As $P$ increases (e.g., $P = 0.5$, $0.7$), the empirical coverage **below** becomes increasingly dominant. \n\n- The model appropriately captures the predictive uncertainty across all cases. \n\n:::\n\n::: \n\n::::: \n\n\n## Prediction, $95\\%$ Predictive Intervals, One-Step Ahead  \n\n::: {.panel-tabset}\n\n### $P = 0.1$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-56-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.1$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-57-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.5$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-58-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.5$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-59-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.7$ $\\epsilon = 0.1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-60-1.png){width=960}\n:::\n:::\n\n\n### $P = 0.7$ $\\epsilon = 0.4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-61-1.png){width=960}\n:::\n:::\n\n\n::: \n\n# Part III Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks {background-color=\"#D73F09\" color=\"white\"} \n\n## Introduction   \n\nRecurrent Neural Networks (RNNs), and their variants, Long Short-Term Memory (LSTMs), are widely used for modeling sequence data because of their ability to capture both short- and long-term dependencies.\n\n. . .\n\nBeyond natural language processing (NLP), RNNs and LSTMs have also shown effective in time series forecasting and have been employed for applications including\n\n- financial market prediction, \n\n- energy forecasting,\n\n- weather and climate modeling, and\n\n- epidemiological trend analysis.\n\n## Introduction and Objectives  \n\nHowever, \n\n- previous studies comparing LSTMs to traditional models such as ARIMA often claim LSTM superiority, a conclusion that can be misleading when the benchmarks chosen are inappropriate.\n\n- A prior study found comparable performance between the MTD model and the LSTM network for predicting disease spread.\n\n. . .\n\n> **Objectives:** Evaluate LSTM and MTD models to ensure a fair and balanced comparison of their performance and robustness. \n\n. . .\n\n> Demonstrate that our proposed MTDs outperform LSTMs in accuracy, though at a higher computational cost and the need for more careful design. \n\n# Jump to Data Application (@sec-data-application)\n\n## RNN (Recurrent Neural Network)\n\n::::: columns\n\n::: column\n\n### \n\n![Architecture of an RNN unit, reproduced from Olah (2015). $x_t$ is the input, $h_t$ is the hidden state, and $o_t$ is the output. $tanh$ is the activation function, squashing values to $(-1, 1)$ for stability and zero-centered output.](images/rnn.png)\n\n::: \n\n::: column\n\n### \n\n- An RNN unit computes a weighted combination of input data, $x_t$, and the previous hidden state, $h_{t-1}$, applies an activation function, and updates the hidden state to $h_t$. \n\n- RNN is composed of repeating units that unfold over time, where each unit passes recurrent information stored in the hidden state from one time step to the next. \n\n- RNN is prone to the well-documented vanishing gradient issue when training on long sequences. \n\n::: \n\n::::: \n\n## LSTM (Long Short-Term Memory)\n\n::::: columns\n\n::: column\n\n### \n\n![Architecture of an LSTM unit with a forget gate, reproduced from Olah (2015). $x_t$ is the input, $h_t$ the hidden state, and $c_t$ the cell state. $f_t$, $i_t$, and $o_t$ are the forget, input, and output gates, respectively. $\\sigma$ is used to squash values to $(0, 1)$ for gating, while $tanh$ squashes values to $(-1, 1)$ for stability and zero-centered output.](images/lstm.png)\n\n::: \n\n::: column\n\n### \n\n- LSTM extends RNN by introducing a cell state and three gates: the forget gate, the input gate, and the output gate. \n\n- The cell state carries long-term dependence, while the hidden state encodes short-term patterns. \n\n- The gates control information flow, deciding what to forget, add, and pass to the hidden state at each step.\n\n::: \n\n::::: \n\n## Training and Hyperparameter Tuning \n\nThe network is typically trained using Backpropagation Through Time (BPTT) and optimized with gradient-based methods such as Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam). \n\n. . .\n\nHyperparameter tuning plays a crucial role in improving model performance. Key hyperparameters include:\n\n- batch size,\n\n- number of epochs,\n\n- learning rate\n\n- number of hidden units or cell dimension, \n\n- number of hidden layers, etc.\n\n# Simulation Studies \n\n## Experimental Setup   \n\nExperiment 1: \n\n- We run both models on Gamma Scenario 1‚Äì9 to compare the predictive performance of the LSTM and MTD models under various conditions. \n\nExperiment 2: \n\n- We run both models on 10 independently generated replicates of Gamma Scenario 1 to assess the stability and robustness of model performance. \n\nExperiment 3: \n\n- We run the LSTM model on 10 independently generated replicates of Gamma Scenario 1 for a variety of configurations (e.g., batch size, learning rate, cell dimention, hidden layer) to investigate the impact of hyperparameters. \n\n## Experimental Setup   \n\nExperiment 4: \n\n- For the zero-inflated Gamma settings, we run both models on Scenario 1 and 2, since each scenario includes six cases defined by all combinations of $P = 0.1, 0.5, 0.7$, and $\\epsilon = 0.1, 0.4$. \n\n## Simulation Results for Gamma Scenarios 1-9\n\n::::: columns\n\n::: column\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.  |LSTM                                                 |MTD                                                  |\n|:--|:----------------------------------------------------|:----------------------------------------------------|\n|s1 |<span style=\" font-weight: bold;    \" >1.3326</span> |1.3569                                               |\n|s2 |2.3001                                               |<span style=\" font-weight: bold;    \" >2.1988</span> |\n|s3 |1.07                                                 |<span style=\" font-weight: bold;    \" >1.0446</span> |\n|s4 |1.6846                                               |<span style=\" font-weight: bold;    \" >1.5282</span> |\n|s5 |<span style=\" font-weight: bold;    \" >1.0215</span> |1.1296                                               |\n|s6 |0.8263                                               |<span style=\" font-weight: bold;    \" >0.7649</span> |\n|s7 |<span style=\" font-weight: bold;    \" >0.7452</span> |0.7617                                               |\n|s8 |<span style=\" font-weight: bold;    \" >0.3675</span> |0.3808                                               |\n|s9 |<span style=\" font-weight: bold;    \" >0.1837</span> |0.1902                                               |\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n- RMSEs for MTD are lower in Scenarios 2, 3, 4, and 6, though the differences are minimal.\n\n- RMSEs are the highest for both models in Scenario 2. \n\n::: \n\n::::: \n\n## Simulation Results for Gamma Scenario 1, 10 replicates\n\n::::: columns\n\n::: column\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-66-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: column\n\n- RMSE has a mean difference of $0.1290$ (`p-value = 0.005175`, `df = 9`), with MTD consistently yielding lower RMSEs. \n\n::: \n\n::::: \n\n## Simulation Results for Gamma Scenario 1, 10 replicates\n\n::::: columns\n\n::: column\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-69-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: column\n\n- Batch size is significant (`Pr(>F) = 4.4e-06`, `df = 2, 24`); RMSEs differ only for $64$ vs. others, and $128$ vs. others.\n\n- Cell dimensions too (`Pr(>F) = 0.0113`, `df = 2, 24`); RMSE differs only between $32$ vs. $64$ and $32$ vs. $128$.\n\n- We adopt the default configuration for subsequent experiments: \n\n  - learning rate = $0.001$, \n  \n  - batch size = $32$, \n  \n  - number of layers = $1$, and \n  \n  - cell dimension = $64$. \n\n::: \n\n::::: \n\n---\n\n![](images/zeros.jpg){.absolute top=0 left=0 width=\"1200\" height=\"675\"}\n\n<div style=\"margin-top: 725px;\">\n¬©Ô∏è Akin Bostanci\n‚è∏Ô∏è ... Quick Visual Break\n</div>\n\n## Simulation Results for ZIGamma Scenario 1\n\n::::: columns\n\n::: column\n\n### \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.        |LSTM                                                 |    MTD|\n|:--------|:----------------------------------------------------|------:|\n|P01Eps01 |<span style=\" font-weight: bold;    \" >1.7221</span> | 1.7302|\n|P01Eps04 |<span style=\" font-weight: bold;    \" >2.0843</span> | 2.6386|\n|P05Eps01 |<span style=\" font-weight: bold;    \" >2.1102</span> | 2.8361|\n|P05Eps04 |<span style=\" font-weight: bold;    \" >2.1788</span> | 2.1922|\n|P07Eps01 |<span style=\" font-weight: bold;    \" >2.2762</span> | 3.0739|\n|P07Eps04 |<span style=\" font-weight: bold;    \" >2.0916</span> | 2.5798|\n\n\n:::\n:::\n\n\n::: \n\n::: column\n\n### \n\n- LSTM generally achieves lower overall RMSEs compared to MTD. \n\n- ‚ùóÔ∏èBut, patterns similar to those in Prediction of Part II (@sec-ch2-prediction) reappear. \n\n\n::: \n\n::::: \n\n## Simulation Results for ZIGamma Scenario 1\n\n::::: columns\n\n::: column\n\n### \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.        | LSTM Below|MTD Below                                            | LSTM Above|MTD Above                                            |\n|:--------|----------:|:----------------------------------------------------|----------:|:----------------------------------------------------|\n|P01Eps01 |     3.6285|4.2964                                               |     1.4910|<span style=\" font-weight: bold;    \" >1.3668</span> |\n|P01Eps04 |     3.2122|5.9082                                               |     1.8826|<span style=\" font-weight: bold;    \" >1.7945</span> |\n|P05Eps01 |     2.0551|<span style=\" font-weight: bold;    \" >0.7752</span> |     2.1655|3.9641                                               |\n|P05Eps04 |     2.0495|<span style=\" font-weight: bold;    \" >1.6675</span> |     2.3426|2.7477                                               |\n|P07Eps01 |     1.2984|<span style=\" font-weight: bold;    \" >0.3677</span> |     3.5798|5.458                                                |\n|P07Eps04 |     1.2583|<span style=\" font-weight: bold;    \" >0.5346</span> |     3.3044|4.6465                                               |\n\n\n:::\n:::\n\n\n::: \n\n::: column\n\n### \n\n::: {.incremental}\n\n- When the zero-inflated probability is low (e.g., $P = 0.1$), the RMSE **above** (which reflects predictive accuracy for values exceeding $\\epsilon$) is a more informative metric for assessing predictive performance. \n\n- As $P$ increases (e.g., $P = 0.5$, $0.7$), this relationship reverses, and the RMSE **below** becomes more relevant. \n\n- MTD outperforms LSTM in RMSE **above** for $P = 0.1$ and again yields lower values for RMSE **below** than LSTM at higher levels of zero-inflation (e.g., $P = 0.5$, $0.7$).\n\n::: \n\n::: \n\n::::: \n\n# Data Application {#sec-data-application}\n\n## Experimental Setup   \n\n::::: columns\n\n::: column\n\n### MTD\n\n- For MTD, the hyperparameter settings are:\n\n  - marginal parameters $\\alpha$, $\\beta$ [^alpha]: $Gamma(\\alpha | u_{\\alpha}, v_{\\alpha} = 1)$, $Gamma(\\beta | u_{\\beta}, v_{\\beta} = 1)$,\n\n  - dependence parameter $\\rho$: $Unif(\\rho_l | -1, 1)$, and\n\n  - weight parameter $w$ [^w]: $CDP(w | \\alpha_0 = 5, a_0 = 1, b_0 = 3)$; cdf-based Dirichlet process prior.\n\n[^alpha]: $u_{\\alpha}$, $u_{\\beta}$ derived from the training data: $u_{\\beta}$ = mean/variance, $u_{\\alpha}$ = mean $\\times$ $u_{\\beta}$.\n\n[^w]: $\\alpha_0$: concentration parameter; $G_0$: base distribution, i.e., $G_0 \\sim Beta(a_0, b_0)$. \n\n- MTD is implemented in `R` and executed on a high-performance computing (HPC) cluster. \n\n::: \n\n::: column\n\n### LSTM\n\n- For LSTM, we reuse the default configuration for these hyperparameters: \n\n  - learning rate = $0.001$, \n  \n  - batch size = $32$, \n  \n  - number of layers = $1$, and \n  \n  - cell dimension = $64$. \n\n- LSTM is implemented in `PyTorch` and trained on a standard workstation.\n\n\n::: \n\n::::: \n\n## Evaluation Metrics [^metrics]\n\n| Metric| Definition | Formula |\n|------|------|------|\n| RMSE    | Root Mean Squared Error    | $\\sqrt{\\frac{1}{T} \\sum_{t=1}^{T} (y_t - \\hat{y}_t)^2}$   |\n| MAE     | Mean Absolute Error     | $\\frac{1}{T} \\sum_{t=1}^{T} |y_t - \\hat{y}_t|$   |\n| MAPE    | Mean Absolute Percentage Error    | $\\frac{100}{T} \\sum_{t=1}^{T} \\left| \\frac{y_t - \\hat{y}_t}{y_t} \\right|$  |\n| SMAPE   | Symmetric MAPE    | $\\frac{100}{T} \\sum_{t=1}^{T} \\frac{|y_t - \\hat{y}_t|}{(|y_t| + |\\hat{y}_t|) / 2}$   |\n| MASE    | Mean Absolute Scaled Error     | $\\frac{ \\frac{1}{T} \\sum_{t=1}^{T} |y_t - \\hat{y}_t| }{ \\frac{1}{T - 1} \\sum_{t=2}^{T} |y_t - y_{t-1}| }$  |\n\n: {tbl-colwidths=\"[25, 50, 50]\"}\n\n[^metrics]: MAPE, SMAPE, and MASE are scale-independent, allowing for comparison across datasets with different units. \n\n## Evaluation Results \n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n### windspeeds at 50m above ground\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.     |    LSTM|MTD                                                  |\n|:-----|-------:|:----------------------------------------------------|\n|RMSE  |  0.6359|<span style=\" font-weight: bold;    \" >0.3508</span> |\n|MAE   |  0.6021|<span style=\" font-weight: bold;    \" >0.2692</span> |\n|MAPE  | 11.1103|<span style=\" font-weight: bold;    \" >4.255</span>  |\n|SMAPE |  9.9305|<span style=\" font-weight: bold;    \" >4.2051</span> |\n|MASE  |  0.6595|<span style=\" font-weight: bold;    \" >0.366</span>  |\n\n\n:::\n:::\n\n\n### windspeeds at 10m above ground\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.     |    LSTM|MTD                                                  |\n|:-----|-------:|:----------------------------------------------------|\n|RMSE  |  0.4607|<span style=\" font-weight: bold;    \" >0.2376</span> |\n|MAE   |  0.3692|<span style=\" font-weight: bold;    \" >0.1614</span> |\n|MAPE  | 11.2891|<span style=\" font-weight: bold;    \" >4.0688</span> |\n|SMAPE | 10.2499|<span style=\" font-weight: bold;    \" >3.9569</span> |\n|MASE  |  0.6194|<span style=\" font-weight: bold;    \" >0.2873</span> |\n\n\n:::\n:::\n\n\n### windspeeds at 2m above ground\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.     |    LSTM|MTD                                                  |\n|:-----|-------:|:----------------------------------------------------|\n|RMSE  |  0.4011|<span style=\" font-weight: bold;    \" >0.2215</span> |\n|MAE   |  0.2696|<span style=\" font-weight: bold;    \" >0.1543</span> |\n|MAPE  | 11.6386|<span style=\" font-weight: bold;    \" >6.5935</span> |\n|SMAPE | 12.6050|<span style=\" font-weight: bold;    \" >6.2548</span> |\n|MASE  |  0.6660|<span style=\" font-weight: bold;    \" >0.3537</span> |\n\n\n:::\n:::\n\n\n::: \n\n## Predicted Means [^caption], One-Step Ahead; Full Test Size $n = 1756$  \n\n::: {.panel-tabset}\n\n### windspeeds at 50m above ground\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-84-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 10m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-85-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 2m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-86-1.png){width=960}\n:::\n:::\n\n\n::: \n\n[^caption]: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means.\n\n## Predicted Means [^caption], One-Step Ahead; Zoom-In View $n = 200$ \n\n::: {.panel-tabset}\n\n### windspeeds at 50m above ground\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-88-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 10m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-89-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 2m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-90-1.png){width=960}\n:::\n:::\n\n\n::: \n\n[^caption]: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means.\n\n## Prediction Errors [^captiondiff], One-Step Ahead; Zoom-In View $n = 200$ \n\n::: {.panel-tabset}\n\n### windspeeds at 50m above ground\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-92-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 10m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-93-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 2m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-94-1.png){width=960}\n:::\n:::\n\n\n::: \n\n[^captiondiff]: Dashed (red) lines show differences between LSTM predicted means and observed values and dashed (blue) lines show differences between MTD predicted means and observed values.\n\n## $95\\%$ Predictive Intervals, One-Step Ahead; Full Test Size $n = 1756$; MTD Only\n\n::::: columns\n\n::: column\n\n### \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|.              | Coverage|\n|:--------------|--------:|\n|windspeed50mms |   0.9522|\n|windspeed10mms |   0.9562|\n|windspeed2mms  |   0.9567|\n\n\n:::\n:::\n\n\n::: \n\n::: column\n\n### \n\n- Empirical coverage is particularly relevant and can only be assessed with probabilistic forecasting methods such as MTD.\n\n- The model appropriately captures the predictive uncertainty across wind speeds at all heights.\n\n::: \n\n::::: \n\n## $95\\%$ Predictive Intervals, One-Step Ahead; Full Test Size $n = 1756$; MTD Only \n\n::: {.panel-tabset}\n\n### windspeeds at 50m above ground\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-99-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 10m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-100-1.png){width=960}\n:::\n:::\n\n\n### windspeeds at 2m above ground\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-101-1.png){width=960}\n:::\n:::\n\n\n::: \n\n---\n\n![](images/conclusion-3.jpg){.absolute top=0 left=0 width=\"1200\" height=\"675\"}\n\n<div style=\"margin-top: 725px;\">\n¬©Ô∏è Getty Images\n‚èØÔ∏è Conclusion\n</div>\n\n## Conclusion {#sec-conclusion}\n\nPart I and II \n\nWe develop a novel copula-based MTD model that separates the dependence structure from the marginal distribution. This separation \n\n- Enables a choice of copula families that effectively capture dependence.\n\n- Provides flexibility in marginal selection.\n\n. . .    \n\nWe also develop a copula-based zero-inflated MTD model, which preserves the advantages of copula modeling for capturing dependence in zero-inflated continuous settings. \n\n- This extension again demonstrates the framework‚Äôs generalizability beyond Gamma marginals, e.g., lognormal, beta .\n\n## Conclusion\n\nPart III\n\nThrough simulation studies and data application,\n\n- MTD models are more robust and explainable but come at the cost of increased computational time and more involved model design. \n\n- LSTMs are general-purpose and efficient, but their black-box nature can limit interpretability and, in some cases, reduce accuracy. \n\n. . .    \n\nOverall, our proposed MTD models provides a flexible, robust, and interpretable approach for modeling complex continuous and zero-inflated time series, demonstrating better performance compared to LSTM in the settings considered. \n\n---\n\n![](images/future.jpg){.absolute top=0 left=0 width=\"1200\" height=\"675\"}\n\n<div style=\"margin-top: 725px;\">\n¬©Ô∏è Shutterstock\n‚èØÔ∏è Future Work\n</div>\n\n## Future Work\n\nPart I and II\n\n- Both the Gamma MTD and ZIGamma MTD models utilize Gaussian copula to model dependence. Future work should explore alternative copulas, like Clayton and Gumbel, to capture tail dependence and asymmetry.\n\n- Although the current framework assumes stationarity and does not incorporate covariates, it can be readily extended by introducing extra model components in the conditional mean structure to enhance flexibility and realism in modeling complex temporal patterns. \n\n- We use MCMC as our primary estimation method, but alternative approaches such as Variational Inference or Sequential Monte Carlo could be explored. \n\n## Future Work\n\nPart III\n\n- Given the growing prominence of transformer architectures in sequence modeling, further research should extend performance comparisons to include transformer-based models. \n\n- It is equally important to ensure that evaluations are conducted using appropriate benchmarks and metrics. \n\n# Thank you!\n\n## Acknowledgement\n\nThank you to `Lisa Madsen` and `Charlotte Wickham`, for your dedication, support, and inspiration throughout the process. \n\nThank you to my committee memebers, `James Molyneux`, `Claudio Fuentes`, and `Prasad Tadepalli`, for the time you dedicated and valuable feedback. \n\nThank you to `Xiaotian Zheng` for being incredibly generous with your time and advices. \n\n. . .    \n\nThank you to `my family`, my mom, dad, and brother, for your occational financial support and for your patience and understanding througout the journey.\n\nThank you to `my friends`, including my cohort, for your emotional support and for always being there, from late-night talks and fun hikes to ski trips and random food adventures.\n\nSpecial thanks to `my partner`, Joseph, for your timely distractions and annoyances. \n\nThank you to many others. \n\n## Acknowledgement\n\n::: {.r-stack}\n\n![](images/a-1.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: 20px; left: -60px; z-index: 1;\"}\n\n![](images/a-2.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: -30px; left: 40px; z-index: 2;\"}\n\n![](images/a-3.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: -60px; left: -30px; z-index: 3;\"}\n\n![](images/a-4-2.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: 30px; left: 60px; z-index: 4;\"}\n\n![](images/a-5-2.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: 20px; left: -60px; z-index: 5;\"}\n\n![](images/a-5.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: -20px; left: -80px; z-index: 6;\"}\n\n![](images/a-6.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: 60px; left: 20px; z-index: 7;\"}\n\n![](images/a-7.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: -80px; left: 10px; z-index: 8;\"}\n\n![](images/a-8.jpg){.fragment width=\"640\" height=\"480\" style=\"position: relative; top: -40px; left: 50px; z-index: 9;\"}\n\n![](images/a-9.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: 30px; left: -50px; z-index: 10;\"}\n\n![](images/a-10.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: -20px; left: 80px; z-index: 11;\"}\n\n![](images/a-11.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: -40px; left: -40px; z-index: 12;\"}\n\n![](images/a-12.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: 40px; left: 50px; z-index: 13;\"}\n\n![](images/a-14.jpg){.fragment width=\"480\" height=\"640\" style=\"position: relative; top: 20px; left: 60px; z-index: 14;\"}\n\n:::\n\n## References\n\nBerchtold, A., & Raftery, A. (2002). \"The mixture transition distribution model for high-order Markov chains and non-Gaussian time series\". *Statistical Science*, Vol. 17 No. 3, pp. 328‚Äì356.\n\nDenuit, Michel, and Lambert, Philippe. 2005. ‚ÄúConstraints on Concordance Measures in Bivariate Discrete Data.‚Äù Journal of Multivariate Analysis 93 (1): 40‚Äì57.\n\nSklar, M (1959). ‚ÄúFonctions de repartition a n dimensions et leurs marges‚Äù, *Annales de l‚ÄôISUP*. 420 Vol. 8. No. 3, pp. 229‚Äì231.\n\nZheng, Xiaotian, Kottas, Athanasios, and Sanso, Bruno (2022). ‚ÄúOn construction and estimation of stationary mixture transition distribution models‚Äù, *Journal of Computational and Graphical Statistics*, Vol. 31 No. 1, pp. 283‚Äì293.\n\n...\n\n# Supplement\n\n## Copula CDF and PDF\n\n![](images/copula-2.png){.absolute top=150 left=250 width=\"780\" height=\"360\"}\n\n<div style=\"margin-top: 650px;\">\nWireframe plot of bivariate Gaussian distribution and density function, reproduced from Kenton (2024). Retrieved from https://www.investopedia.com/terms/c/copula.asp.\n</div>\n\n## Copula Families \n\n![](images/copula_fam-2.png){.absolute top=100 left=0 width=\"1200\" height=\"600\"}\n\n<div style=\"margin-top: 650px;\">\nContour plot of different bivariate density function where marginals are $N(0, 1)$, reproduced from Chang (2019). Retrieved from https://bochang.me/blog/posts/copula/. \n</div>\n\n## Sklar‚Äôs Theorem\n\n::: {.proposition #prop-copula}\nFor any $p$-dimensional multivariate cumulative distribution function (cdf) of a random vector $(X_1, ..., X_p)$, denoted as $F(x_1, ..., x_p)$, there exists a copula function $C: {[0,1]}^p \\rightarrow [0, 1]$ for which $F(x_1, ..., x_p) = C(F_1(x_1), ..., F_p(x_p))$, where $F_j$ is the marginal cdf of $X_j, j = 1,..., p$. If $X_j$ is continuous for all $j$, then $C$ is unique and differentiable. The joint probability density function (pdf) of $X_j, j = 1,...,p$ is given by $f(x_j) = c(x_j) \\prod_{j=1}^p f_j(x_j)$, where $c = \\partial^p{C} / \\partial{F_j}$ is the copula density and $f_j$ is the density of $X_j$.\n::: \n\n## Proposition 1: Zheng et al. (2022)'s Invariant Condition\n\n::: {.proposition #prop-1}\nConsider a set of bivariate random vectors $(U_l, V_l)$ that takes values in $S \\times S \\subset \\mathbb{R}$. Consider a time series $\\{ X_t: t \\in \\mathbb{N} \\}$, where $X_t \\in S$, generated from\n$$\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^L w_l f_{U_l|V_l} (x_t | x_{t-l}), t > L, \n$$\nand from \n$$\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^{t-2} w_l f_{U_l|V_l} (x_t | x_{t-l}) + \n\\left( 1 - \\sum_{k=1}^{t-2} w_k \\right) f_{U_{t-1} | V_{t-1}} (x_t|x_1), 2 \\leq t \\leq L.\n$$\n\nIf a time series satisfies the invariant condition: $X_1 \\sim f_X$, and $f_{U_l}(x) = f_{V_l}(x) = f_X(x)$, for all $x \\in S$, and for all $l$, then this time time series is first-order strictly stationary with invariant marginal density $f_X$.\n::: \n\n## Shifted Gamma Distribution \n\nThe shifted Gamma distribution, $\\text{ShiftedGamma}(\\mu, \\beta; \\epsilon)$, is expressed as: \n```{=latex} \n\\begin{equation}\nf(y; \\mu, \\beta, \\epsilon) = \n\\frac{1} {\\Gamma(\\frac{\\mu}{\\beta}) \\beta^{\\frac{\\mu}{\\beta}}} {(y - \\epsilon)}^{\\frac{\\mu}{\\beta} - 1} \\exp(- \\frac{y - \\epsilon} {\\beta}) \\quad y \\geq \\epsilon, \n\\end{equation}\n```\nwhere $\\frac{\\mu}{\\beta} > 0$ denotes the shape, $\\beta > 0$ the scale, and $\\epsilon > 0$ the threshold parameter.\n\n## Algorithm 1: Implementaion Details \n\n- The full Bayesian model is complete by priors specification for the parameters $\\boldsymbol{\\theta} = \\{\\alpha, \\beta, \\boldsymbol{\\rho}\\}$ and $\\boldsymbol{w}$.\n\n- The posterior full conditional distributions:\n\n  - For the marginal parameters $\\alpha$ and $\\beta$ are proportional to $Gamma(\\alpha | u_{\\alpha}, v_{\\alpha}) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})$ and $Gamma(\\beta | u_{\\beta}, v_{\\beta}) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})$, respectively. $f_l (x_t | x_{t-l}) = c_l (x_t, x_{t-l}) f_X(x_t)$. \n  \n  - For each of the dependence parameters $\\boldsymbol{\\rho}$ is proportional to $\\text{Unif}(\\rho_l |-1, 1) \\prod_{t:z_t = l} c_l (x_t, x_{t-l})$. \n  \n  - For each $z_t$ of the latent variables ${\\{z_t\\}}_{t=L+1}^n$, where the probability of $z_t = l$, denoted by $q_l$, is proportional to $w_l c_l (x_t, x_{t-l})$, for $l = 1,..., L$. \n\n  - For weight parameters $\\boldsymbol{w}$, under the cdf-based prior, is $Dirichlet (\\boldsymbol{\\alpha})$, where $\\boldsymbol{\\alpha} = (\\alpha_0 a_1 + M_1, ..., \\alpha_0 a_L + M_L)$. $a_l = G_0(l/L) - G_0((l - 1)/L)$, $M_l = |\\{ t: z_t = l \\}|$. \n\n--- \n\n![Asterisk (*) denotes steps that differ from Zheng's (2022) algorithm.](images/alg-1.png){fig-align=\"left\"}\n\n## Algorithm 2: Implementaion Details \n\n- The full Bayesian model is complete by priors specification for the parameters $\\boldsymbol{\\theta} = \\{\\alpha, \\beta, P, \\epsilon, \\boldsymbol{\\rho}\\}$ and $\\boldsymbol{w}$.\n\n- The posterior full conditional distributions:\n\n  - (Same as Gamma MTD)\n\n  - For the zero-inflated probability $P$ is proportional to $Unif(P | 0, 1) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})$.\n\n  - For the threshold parameter $\\epsilon$ is proportional to $ScaledBeta(5, 5; 0, 2\\epsilon_0) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})$.\n\n\n--- \n\n![Asterisk (**) denotes denotes steps that differ from Algorithm 1.](images/alg-2.png){fig-align=\"left\"}\n\n## Sensitivity Analysis \n\nSensitivity Analysis\n\n- Use five different sets of priors (i.e., fix one parameter, and vary the other with informative, diffuse or shifted prior) and check for convergence. \n\n- Results appear reasonable, and the estimates are consistent with the true values, indicating that the model is robust to the choice of prior. \n\n## Sensitivity Analysis for Gamma Scenario 1\n\n| Prior for $\\alpha$ | Prior for $\\beta$ | Description |\n|------|------|------|\n| $Gamma(49, 7)$    | $Gamma(1, 1)$    | Informative prior for both $\\alpha$ and $\\beta$. |\n| $Gamma(4.9, 0.7)$ | $Gamma(1, 1)$    |Diffuse prior for $\\alpha$. Informative prior for $\\beta$. |\n| $Gamma(49, 7)$  | $Gamma(0.1, 0.1)$ | Informative prior for $\\alpha$. Diffuse prior for $\\beta$. |\n| $Gamma(10, 1)$ | $Gamma(1, 1)$ | Shifted prior for $\\alpha$. Informative prior for $\\beta$. |\n| $Gamma(49, 7)$ | $Gamma(4, 1)$  | Informative prior for $\\alpha$. Shifted prior for $\\beta$. |\n\n: {tbl-colwidths=\"[25, 25, 50]\"}\n\n## Sensitivity Analysis for Gamma Scenario 1\n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n### $\\alpha$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-103-1.png){width=960}\n:::\n:::\n\n\n### $\\beta$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-104-1.png){width=960}\n:::\n:::\n\n\n::: \n\n## Coverage Assessment \n\nCoverage Assessment\n\n- Simulate $40$ replicates, combine $4$ chains of $8,000$ samples, calculate $95\\%$ CI, and record whether the true parameter value falls within this interval. \n\n- Most parameters achieve full coverage, with a few slightly below $1$, indicating that the credible intervals reliably capture the true parameter values. \n\n## Coverage Assessment for Scenario 1 and 2, 40 Replicates \n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### Exponentially decreasing weights (s1)\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| $\\alpha$| $\\beta$| $w_1$| $w_2$| $w_3$| $w_4$| $w_5$| $\\rho_1$| $\\rho_2$| $\\rho_3$| $\\rho_4$| $\\rho_5$|\n|--------:|-------:|-----:|-----:|-----:|-----:|-----:|--------:|--------:|--------:|--------:|--------:|\n|     0.95|    0.95| 0.975| 0.975|     1|     1|     1|      0.9|    0.975|        1|    0.975|        1|\n\n\n:::\n:::\n\n\n### Uneven arrangement of the relevant lags (s2)\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| $\\alpha$| $\\beta$| $w_1$| $w_2$| $w_3$| $w_4$| $w_5$| $\\rho_1$| $\\rho_2$| $\\rho_3$| $\\rho_4$| $\\rho_5$|\n|--------:|-------:|-----:|-----:|-----:|-----:|-----:|--------:|--------:|--------:|--------:|--------:|\n|     0.95|    0.95|     1|     1| 0.975| 0.975|  0.95|        1|    0.975|     0.95|        1|     0.95|\n\n\n:::\n:::\n\n\n::: \n\n## Coverage Assessment for Scenario 1, 40 Replicates \n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n### $\\alpha$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-110-1.png){width=960}\n:::\n:::\n\n\n### $\\beta$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-111-1.png){width=960}\n:::\n:::\n\n\n::: \n\n## Coverage Assessment for Scenario 1, 40 Replicates \n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n### $w_1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-113-1.png){width=960}\n:::\n:::\n\n\n### $w_2$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-114-1.png){width=960}\n:::\n:::\n\n\n### $w_3$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-115-1.png){width=960}\n:::\n:::\n\n\n### $w_4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-116-1.png){width=960}\n:::\n:::\n\n\n### $w_5$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-117-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Coverage Assessment for Gamma Scenario 1, 40 Replicates \n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n### $\\rho_1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-119-1.png){width=960}\n:::\n:::\n\n\n### $\\rho_2$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-120-1.png){width=960}\n:::\n:::\n\n\n### $\\rho_3$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-121-1.png){width=960}\n:::\n:::\n\n\n### $\\rho_4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-122-1.png){width=960}\n:::\n:::\n\n\n### $\\rho_5$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-123-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Prediction, $95\\%$ Predictive Intervals, One-Step Ahead for Gamma Scenario 1-9\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|         |     s1|     s2|     s3|     s4|     s5|     s6|     s7|     s8|     s9|\n|:--------|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n|Coverage | 0.9544| 0.9519| 0.9534| 0.9459| 0.9544| 0.9474| 0.9524| 0.9444| 0.9544|\n\n\n:::\n:::\n\n\n## Trace and Density Plot for ZIGamma Scenario 1‚Äôs $w$\n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### $w_1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-128-1.png){width=960}\n:::\n:::\n\n\n### $w_2$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-129-1.png){width=960}\n:::\n:::\n\n\n### $w_3$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-130-1.png){width=960}\n:::\n:::\n\n\n### $w_4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-131-1.png){width=960}\n:::\n:::\n\n\n### $w_5$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-132-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Trace and Density Plot for ZIGamma Scenario 1‚Äôs $\\rho$\n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### $\\rho_1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-135-1.png){width=960}\n:::\n:::\n\n\n### $\\rho_2$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-136-1.png){width=960}\n:::\n:::\n\n\n### $\\rho_3$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-137-1.png){width=960}\n:::\n:::\n\n\n### $\\rho_4$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-138-1.png){width=960}\n:::\n:::\n\n\n### $\\rho_5$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-139-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Trace and Density Plot for ZIGamma Scenario 1's $\\mu, \\beta, P, \\epsilon$\n\n::: {.panel-tabset}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### $\\mu$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-142-1.png){width=960}\n:::\n:::\n\n\n### $\\beta$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-143-1.png){width=960}\n:::\n:::\n\n\n### $P$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-144-1.png){width=960}\n:::\n:::\n\n\n### $\\epsilon$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-145-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## RNN \n\nLet $x_t$, $h_t$, and $o_t$ denote the input data, the hidden state, and the output at time $t$, respectively. Then, an RNN unit can be expressed as: \n```{=latex} \n\\begin{equation}\n\\begin{split}\n\\label{eq:rnn}\nh_t &= f(W_{ih} h_{t-1} + W_{ix} x_t + b_i), \\\\\no_t &= g(W_{oh} \\cdot h_t + b_o), \n\\end{split}\n\\end{equation}\n```\nwhere  $W_{ix}$, $W_{ih}$ and $W_{oh}$ denote the weight matrices, and $b_i$ and $b_o$ the bias vectors. \n\nThe subscripts $i$ and $o$ indicate their steps in RNN: $i$ refer to the input/hidden step, and $o$ to the output step. \n\n$f$ and $g$ denote the activation functions for the hidden layer and output layer, respectively. \n\n- $f$ is typically set to the logistic sigmoid function, denoted as $\\sigma$, which outputs values in range $(0, 1)$ to act as a gate that controls how much information passes through. \n\n- $g$ is the hyperbolic tangent function, denoted as $tanh$, which outputs values in range $(-1, 1)$ to generate output in a stable, zero-centered range. \n\n## LSTM\n\nLet $c_t$ and $h_t$ denote the cell and the hidden state vector. Let $f_t$, $i_t$, and $o_t$ represent the forget, the input, and the output gate vector at time $t$, respectively. Then, an LSTM unit can be expressed as: \n```{=latex} \n\\begin{equation}\n\\begin{split}\n\\label{eq:lstm}\nf_t &= \\sigma(W_{fh} h_{t-1} + W_{fx} x_t + b_f), \\\\\ni_t &= \\sigma(W_{ih} h_{t-1} + W_{ix} x_t + b_i), \\\\\n\\tilde{c}_t &= \\tanh(W_{\\tilde{c} h} h_{t-1} + W_{\\tilde{c} x} x_t + b_{\\tilde{c}}), \\\\\nc_t &= f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t, \\\\\no_t &= \\sigma(W_{oh} h_{t-1} + W_{ox} x_t + b_o), \\\\\nh_t &= o_t \\cdot \\tanh(c_t), \n\\end{split}\n\\end{equation}\n```\nwhere $\\boldsymbol{W}$ denotes the weight matrices, $\\boldsymbol{b}$ the bias vectors, $\\sigma$ the logistic sigmoid function, and $tanh$ the hyperbolic tangent function. \n\n## Training and Validation Loss Curves for LSTM\n\n![](images/p_loss_real_bs81632_v2.png)\n\n## Training and Validation Loss Curves for LSTM\n\n![](images/p_loss_real_bs64128256_v2.png)",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}