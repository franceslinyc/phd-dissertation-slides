[
  {
    "objectID": "graph.html",
    "href": "graph.html",
    "title": "",
    "section": "",
    "text": "X1\n\nX‚ÇÅ\n\n\n\nX2\n\nX‚ÇÇ\n\n\n\nX1--X2\n\n\n\n\nX3\n\nX‚ÇÉ\n\n\n\nX2--X3\n\n\n\n\nX4\n\nX‚ÇÑ\n\n\n\nX3--X4\n\n\n\n\nX5\n\nX‚ÇÖ\n\n\n\nX4--X5\n\n\n\n\nX6\n\nX‚ÇÜ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX1\n\nX‚ÇÇ\n\n\n\nX2\n\nX‚ÇÉ\n\n\n\nX1--X2\n\n\n\n\nX3\n\nX‚ÇÑ\n\n\n\nX2--X3\n\n\n\n\nX4\n\nX‚ÇÖ\n\n\n\nX3--X4\n\n\n\n\nX5\n\nX‚ÇÜ\n\n\n\nX4--X5\n\n\n\n\nX6\n\nX‚Çá\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX1\n\n \nX\nt-5\n \n\n\n\nX2\n\n \nX\nt-4\n \n\n\n\nX1--X2\n\n\n\n\nX3\n\n \nX\nt-3\n \n\n\n\nX2--X3\n\n\n\n\nX4\n\n \nX\nt-2\n \n\n\n\nX3--X4\n\n\n\n\nX5\n\n \nX\nt-1\n \n\n\n\nX4--X5\n\n\n\n\nX6\n\n \nX\nt"
  },
  {
    "objectID": "metric.html",
    "href": "metric.html",
    "title": "",
    "section": "",
    "text": "Metric\nDefinition\nFormula\n\n\n\n\nRMSE\nRoot Mean Squared Error\n\\(\\sqrt{\\frac{1}{T} \\sum_{t=1}^{T} (y_t - \\hat{y}_t)^2}\\)\n\n\nMAE\nMean Absolute Error\n\\(\\frac{1}{T} \\sum_{t=1}^{T} |y_t - \\hat{y}_t|\\)\n\n\nMAPE\nMean Absolute Percentage Error\n\\(\\frac{100}{T} \\sum_{t=1}^{T} \\left| \\frac{y_t - \\hat{y}_t}{y_t} \\right|\\)\n\n\nSMAPE\nSymmetric MAPE\n\\(\\frac{100}{T} \\sum_{t=1}^{T} \\frac{|y_t - \\hat{y}_t|}{(|y_t| + |\\hat{y}_t|) / 2}\\)\n\n\nMASE\nMean Absolute Scaled Error\n\\(\\frac{ \\frac{1}{T} \\sum_{t=1}^{T} |y_t - \\hat{y}_t| }{ \\frac{1}{T - 1} \\sum_{t=2}^{T} |y_t - y_{t-1}| }\\)"
  },
  {
    "objectID": "index.html#copula-based-mixture-transition-distribution-models-for-forecasting-skewed-and-zero-inflated-time-series-methodology-and-comparisons-with-deep-learning-lstm-networks",
    "href": "index.html#copula-based-mixture-transition-distribution-models-for-forecasting-skewed-and-zero-inflated-time-series-methodology-and-comparisons-with-deep-learning-lstm-networks",
    "title": "",
    "section": "Copula-Based Mixture Transition Distribution Models for Forecasting Skewed and Zero-Inflated Time Series: Methodology and Comparisons with Deep Learning LSTM Networks",
    "text": "Copula-Based Mixture Transition Distribution Models for Forecasting Skewed and Zero-Inflated Time Series: Methodology and Comparisons with Deep Learning LSTM Networks\n\n\n\n\n\nFrances Lin\nüõ° Dissertation Defense\nü¶´ Oregon State University\nNovember 5, 2025\n\n\nCo-Advisors: Dr.¬†Lisa Madsen, Dr.¬†Charlotte Wickham\nCommittee Members: Dr.¬†James Molyneux, Dr.¬†Claudio Fuentes, Dr.¬†Prasad Tadepalli"
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "",
    "section": "Motivation",
    "text": "Motivation\nModeling complex patterns in sequence data is a central task across domains such as energy, insurance, and transportation.\n\n‚ö†Ô∏è Real-world time series often show skewness and zero inflation, which can hinder prediction if ignored.\n\n\nRecent AI advances, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, effectively capture nonlinear and long-range dependence.\n\n\n\n‚ö†Ô∏è Claims of LSTM superiority can be misleading when compared to inappropriate benchmarks.\n‚ö†Ô∏è A prior work found comparable performance between probabilistic Mixture Transition Distribution (MTD) model and deep learning LSTM for predicting disease spread."
  },
  {
    "objectID": "index.html#nasa-merra-2-data",
    "href": "index.html#nasa-merra-2-data",
    "title": "",
    "section": "NASA MERRA-2 Data",
    "text": "NASA MERRA-2 Data\n\nwindspeeds at 50m above groundwindspeeds at 10m above groundwindspeeds at 2m above ground"
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "",
    "section": "Objectives",
    "text": "Objectives\nPart I and II\n\nDevelop the copula-based Gamma Mixture Transition Distribution (Gamma MTD) model and its zero-inflated extension (ZIGamma MTD) to capture nonlinear dependence, skewed features, and semicontinuous patterns.\n\n\nPart III\n\nEvaluate the proposed models alongside deep learning approaches, specifically the Long Short-Term Memory (LSTM) networks, demonstrating superior predictive performance and robustness."
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "",
    "section": "Roadmap",
    "text": "Roadmap\nPart I Models for Forecasting Skewed Time Series\n\nRecap of MTD Models\nSensitivity Analysis\nCoverage Assessment\n\nPart II Models for Forecasting Zero-Inflated Skewed Time Series\n\nIntroduction\nBackground\nProposed Method\nSimulation Studies\nPrediction"
  },
  {
    "objectID": "index.html#roadmap-1",
    "href": "index.html#roadmap-1",
    "title": "",
    "section": "Roadmap",
    "text": "Roadmap\nPart III Copula-Based Probabilistic MTD Models vs.¬†Deep Learning LSTM Networks\n\nIntroduction\nBackground\nHyperparameter Tuning, Training, and Metrics\nSimulation Studies\nData Application"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nTime series model captures how past values contribute to current value and uses this information to predict future value.\n\nThe mixture transition distribution (MTD) models\n\nExtend the autoregressive (AR) models to accommodate discrete, continuous, and mixed time series.\nModel each conditional distribution as a mixture of transition kernels, with random, stochastic weights."
  },
  {
    "objectID": "index.html#introduction-and-objectives",
    "href": "index.html#introduction-and-objectives",
    "title": "",
    "section": "Introduction and Objectives",
    "text": "Introduction and Objectives\nOur work builds upon the architecture of the MTD model introduced by Zheng et al.¬†(2022).\n\nIncludes various applications, such as Gaussian, Poisson, negative binomial, and Lomax regression MTD models, extending beyond linear, Gaussian dynamics.\n\n\nHowever, for certain invariant marginal distributions, the transition kernel may either require careful construction or can result in a form that is not explicitly defined or too complex.\n\n\n\nObjectives: Propose to incorporate copulas into the transition kernels to address this limitation, thereby enhancing modeling capabilities and flexibility.\n\n\n\n\nDevelop the Gamma MTD model, but the framework is generalizable."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "",
    "section": "Background",
    "text": "Background\nThe MTD model is a state space model.\n\nThe MTD model was initially developed in 1985 to model high-order Markov chains,\n\nFollowed by several variant models over the years to capture non-Gaussian and nonlinear features, such as flat stretches, bursts, outliers, and change points.\nSee Berchtold and Raftery (2022) for a complete review.\n\n\n\nThe MTD model has primarily been used for modeling time series.\n\nIts generalizations has also demonstrated success in modeling spatial data."
  },
  {
    "objectID": "index.html#model-framework-dag-also-known-as-bayesian-network",
    "href": "index.html#model-framework-dag-also-known-as-bayesian-network",
    "title": "",
    "section": "Model Framework: DAG; also known as Bayesian network",
    "text": "Model Framework: DAG; also known as Bayesian network\nLet \\(\\{ X_t: t \\in \\mathbb{N} \\}\\) be a time series. Construct the model on a DAG, the joint distribution of \\(X_1\\) to \\(X_t\\) can be factorized into a product of conditional distributions as\n\\[\\begin{equation}\nf(\\textbf{x}) = f(x_1) \\prod_{t=2}^t f(x_t | \\textbf{x}^{t-1}). \\tag{1}\n\\label{eq:joint_distribution}\n\\end{equation}\\]\n\\(f(x_t | \\textbf{x}^{t-1})\\) is the conditional probability density function (pdf) of current value \\(X_t\\) given all of its past values \\(\\textbf{X}^{t-1} = \\textbf{x}^{t-1}\\), where \\(\\textbf{X}^{t-1} = \\{X_i: i \\leq t - 1 \\}\\) and \\(\\textbf{x}^{t-1} = \\{x_i: i \\leq t - 1 \\}\\).\n\nAs \\(t\\) increases, the size of the conditioning set of \\(X_t\\) can be large.\n\nSolution: Use a structured mixture model that restricts each value in the MTD model to depend only on the \\(L\\) (a given number) lagged values.\n\n\n\nEach conditional in (1) is modeled as a mixture of transition kernels, with weight assigned to each of the \\(L\\) lagged values."
  },
  {
    "objectID": "index.html#mtd-order-look-back-steps-l-5",
    "href": "index.html#mtd-order-look-back-steps-l-5",
    "title": "",
    "section": "MTD order (look-back steps) \\(L = 5\\)",
    "text": "MTD order (look-back steps) \\(L = 5\\)\n\n\n\n\n\n\n\n\n\n\nX1\n\nX‚ÇÅ\n\n\n\nX2\n\nX‚ÇÇ\n\n\n\nX1--X2\n\n\n\n\nX3\n\nX‚ÇÉ\n\n\n\nX2--X3\n\n\n\n\nX4\n\nX‚ÇÑ\n\n\n\nX3--X4\n\n\n\n\nX5\n\nX‚ÇÖ\n\n\n\nX4--X5\n\n\n\n\nX6\n\nX‚ÇÜ"
  },
  {
    "objectID": "index.html#mtd-order-look-back-steps-l-5-1",
    "href": "index.html#mtd-order-look-back-steps-l-5-1",
    "title": "",
    "section": "MTD order (look-back steps) \\(L = 5\\)",
    "text": "MTD order (look-back steps) \\(L = 5\\)\n\n\n\n\n\n\n\n\n\n\nX1\n\nX‚ÇÇ\n\n\n\nX2\n\nX‚ÇÉ\n\n\n\nX1--X2\n\n\n\n\nX3\n\nX‚ÇÑ\n\n\n\nX2--X3\n\n\n\n\nX4\n\nX‚ÇÖ\n\n\n\nX3--X4\n\n\n\n\nX5\n\nX‚ÇÜ\n\n\n\nX4--X5\n\n\n\n\nX6\n\nX‚Çá"
  },
  {
    "objectID": "index.html#mtd-order-look-back-steps-l-5-2",
    "href": "index.html#mtd-order-look-back-steps-l-5-2",
    "title": "",
    "section": "MTD order (look-back steps) \\(L = 5\\)",
    "text": "MTD order (look-back steps) \\(L = 5\\)\n\n\n\n\n\n\n\n\n\n\nX1\n\n \nX\nt-5\n \n\n\n\nX2\n\n \nX\nt-4\n \n\n\n\nX1--X2\n\n\n\n\nX3\n\n \nX\nt-3\n \n\n\n\nX2--X3\n\n\n\n\nX4\n\n \nX\nt-2\n \n\n\n\nX3--X4\n\n\n\n\nX5\n\n \nX\nt-1\n \n\n\n\nX4--X5\n\n\n\n\nX6\n\n \nX\nt"
  },
  {
    "objectID": "index.html#model-framework-transition-kernels-mixture-weights",
    "href": "index.html#model-framework-transition-kernels-mixture-weights",
    "title": "",
    "section": "Model Framework: Transition kernels & mixture weights",
    "text": "Model Framework: Transition kernels & mixture weights\nFor \\(t &gt; L\\), the MTD model specifies the conditional distribution of \\(X_t\\) given \\(\\textbf{X}^{t-1} = \\textbf{x}^{t-1}\\) as\n\\[\\begin{equation}\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^L w_l f_l (x_t | x_{t-l}). \\tag{2}\n\\label{eq:cond_distribution}\n\\end{equation}\\]\n\\(f_l (x_t | x_{t-l})\\) is the conditional pdf of \\(X_t\\) with respect to the \\(l\\)th transition kernel given that \\({X}_{t-l} = {x}_{t-l}\\). \\(w_l\\) are weight parameters, where \\(w_l \\geq 0\\) such that \\(\\sum_{l=1}^L w_l = 1\\).\n\nThere are two components in (2):\n\nTransition kernel, \\(f_l\\), captures the influence of the \\(l\\)th lag value on the current value.\nWeight parameter, \\(w_l\\), determines the relative contribution of that influence."
  },
  {
    "objectID": "index.html#copula",
    "href": "index.html#copula",
    "title": "",
    "section": "Copula",
    "text": "Copula\nCopula is widely used for dependence modeling.\n\nIs applied in quantitative finance, reliability engineering, hydrology, etc.\nConsists of families of copula, constituting a substantial research area.\n\n\nCopula is a multivariate cumulative distribution function (cdf) where its marginal distribution of each random variables is \\(Unif(0, 1)\\).\n\n\nUsing copula, any joint distribution, \\(F\\), can be decomposed into two parts: the copula, \\(C\\), and the marginal distributions, \\(F_j\\), \\(j = 1,..., p\\).\n\nRooted in Sklar‚Äôs theorem."
  },
  {
    "objectID": "index.html#existing-method-to-proposed-method",
    "href": "index.html#existing-method-to-proposed-method",
    "title": "",
    "section": "Existing Method to Proposed Method",
    "text": "Existing Method to Proposed Method\nThe bivariate distribution approach, for example, identifies a bivariate distribution of \\((U_l, V_l)\\) such that the marginal densities, \\(f_{U_l}\\) and \\(f_{V_l}\\), are equal to a pre-specified stationary marginal density \\(f_X\\) for all \\(l\\) transition kernels.\n\nBased on Proposition 1 of Zheng et al.¬†(2022).\n\n\nThis is facilitated by the use of a copula, which separates the marginal behavior of the random variables from their dependence structure.\n\\[\n\\textbf{transition kernel} \\longleftarrow \\textbf{copula} \\times \\textbf{marginal}\n\\] \\[\n\\ f_l (x_t | x_{t-l}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ c_l (x_t, x_{t-l}) \\ \\ \\ \\ f_X (x_t)\n\\]"
  },
  {
    "objectID": "index.html#proposed-method",
    "href": "index.html#proposed-method",
    "title": "",
    "section": "Proposed Method",
    "text": "Proposed Method\nReplace \\(f_{U_l}\\) and \\(f_{V_l}\\) with a pre-specified stationary marginal density, \\(f_X\\), for every \\(x_t\\) and for all \\(l\\). For \\(t &gt; L\\), the proposed copula-based MTD model specifies the conditional distribution as\n\\[\\begin{equation}\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^L w_l c_l (x_t, x_{t-l}) f_X(x_t). \\tag{3}\n\\label{eq:cond_distribution_copula}\n\\end{equation}\\]\n\\(c_l (x_t, x_{t-l})\\) is the copula density evaluated at \\(x_t\\) and \\(x_{t-l}\\), and \\(f_X(x_t)\\) is the stationary marginal density evaluated at \\(x_t\\).\n\nCompared to (2), the transition kernel, \\(f_l\\), is now replaced by two components:\n\nCopula density, \\(c_l\\), captures and controls the strength of the dependence through a dependence parameter.\nStationary marginal density, \\(f_X\\), describes the marginal behavior through distribution parameters."
  },
  {
    "objectID": "index.html#background-bayesian-implementation",
    "href": "index.html#background-bayesian-implementation",
    "title": "",
    "section": "Background: Bayesian Implementation",
    "text": "Background: Bayesian Implementation\nEarlier MTD models were built through frequentist approaches.\nEstimation and prediction in the MTD model by Zheng et al.¬†(2022) is constructed with Bayesian methodologies.\n\nOur proposed copula-based MTD model builds upon this model version.\nWe consider the Gaussian copula with Gamma marginals, i.e., Gamma MTD model.\n\nBut the framework is generalizable, e.g., Lognormal MTD model or for any continuous distribution."
  },
  {
    "objectID": "index.html#background-bayesian-implementation-1",
    "href": "index.html#background-bayesian-implementation-1",
    "title": "",
    "section": "Background: Bayesian Implementation",
    "text": "Background: Bayesian Implementation\nInference is facilitated through a set of latent variables \\({\\{Z_t\\}}_{t=L+1}^n\\). \\(Z_t = l\\) indicates which \\(l\\)th kernel or component the data belongs to.\nThe posterior distribution of the parameters, based on the conditional likelihood, is\n\\[\\begin{equation}\n\\begin{split}\np(\\boldsymbol{w}, \\boldsymbol{\\theta}, {\\{z_t\\}}_{t=L+1}^n | D_n) \\propto \\pi_w(\\boldsymbol{w}) \\prod_{l=1}^L \\pi_l(\\boldsymbol{\\theta}_l) \\\\\n\\prod_{t=L+1}^n \\Biggl\\{ f_{z_t} (x_t | x_{t-z_t}, \\boldsymbol{\\theta}_{z_t}) \\sum_{l=1}^L w_l \\delta_l(z_t) \\Biggl\\}.\n\\end{split} \\tag{4}\n\\end{equation}\\]\n\\(\\boldsymbol{\\theta}\\) collects parameters for the transition kernel. \\(D_n = {\\{x_t\\}}_{t=L+1}^n\\) is the data.\n\nWith priors specifications, the full simulation-based Bayesian estimation and prediction can be achieved using Markov chain Monte Carlo (MCMC) algorithms."
  },
  {
    "objectID": "index.html#simulation-studies",
    "href": "index.html#simulation-studies",
    "title": "",
    "section": "Simulation Studies",
    "text": "Simulation Studies\nWe have assessed accuracy and performance of the proposed model.\n\nConvergence Diagnostics (\\(\\boldsymbol{w}\\), \\(\\boldsymbol{\\rho}\\), \\(\\alpha\\), \\(\\beta\\))\nWeight and Dependence Parameters for Copula (\\(\\boldsymbol{w}\\), \\(\\boldsymbol{\\rho}\\))\nParameters for Marginal Distribution (\\(\\alpha\\), \\(\\beta\\))\n\n\nSimulation studies demonstrate promising results across various scenarios.\n\nScenario 1 and 2: Default setup; Compatible \\(\\boldsymbol{w}\\) and \\(\\boldsymbol{\\rho}\\)\nScenario 1.3 and 1.4: Incompatible \\(\\boldsymbol{w}\\) and \\(\\boldsymbol{\\rho}\\)\nScenario 1.5 and 1.6: Compatible \\(\\boldsymbol{w}\\) and \\(\\boldsymbol{\\rho}\\); Rarely observable patterns\nScenario 3-6: Usual case for gamma marginals\nScenario 7-9: Unusual case involving high skewness"
  },
  {
    "objectID": "index.html#weight-and-dependence-parameters-for-copula",
    "href": "index.html#weight-and-dependence-parameters-for-copula",
    "title": "",
    "section": "Weight and Dependence Parameters for Copula",
    "text": "Weight and Dependence Parameters for Copula\n\nExponentially decreasing weights (s1)Uneven arrangement of the relevant lags (s2)"
  },
  {
    "objectID": "index.html#marginal-distribution",
    "href": "index.html#marginal-distribution",
    "title": "",
    "section": "Marginal Distribution",
    "text": "Marginal Distribution\n\nUsual Cases (s3-6)Unusual Cases (s7-9)"
  },
  {
    "objectID": "index.html#sensitivity-analysis-and-coverage-assessment",
    "href": "index.html#sensitivity-analysis-and-coverage-assessment",
    "title": "",
    "section": "Sensitivity Analysis and Coverage Assessment",
    "text": "Sensitivity Analysis and Coverage Assessment\nSince the preliminary exam, we have also conducted:\n\nSensitivity Analysis\nCoverage Assessment\n\n\nResults appear reasonable, and the estimates are consistent with the true values, indicating that the model is robust to the choice of prior.\nMost parameters achieve full coverage, indicating that the credible intervals reliably capture the true parameter values."
  },
  {
    "objectID": "index.html#introduction-1",
    "href": "index.html#introduction-1",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nZero-inflated data are characterized by an excess of zero values and frequently appear in domains such as healthcare, insurance, environment, and transportation.\nExamples include\n\nmedical costs,\ninsurance claims,\nprecipitation amounts, and\nsafety measures.\n\nIf left unaddressed, zero-inflation undermines model robustness and weakens inference."
  },
  {
    "objectID": "index.html#introduction-and-objectives-1",
    "href": "index.html#introduction-and-objectives-1",
    "title": "",
    "section": "Introduction and Objectives",
    "text": "Introduction and Objectives\nIn Part I, we propose the copula-based Gamma MTD model, which enables flexible dependence modeling and accommodates arbitrary continuous marginals.\nHowever,\n\nit remains limited in handling excessive zeros commonly observed in real-world continuous data.\nCopula-based approaches face identifiability issues when modeling discrete or mixed marginals.\n\n\n\nObjectives: Propose reconstructing the marginal distribution to account for zero-inflation, while maintaining the same effectiveness and flexibility in modeling dependence structures as described in Part I.\n\n\n\n\nDevelop the ZIGamma MTD model, but the framework is generalizable."
  },
  {
    "objectID": "index.html#background-1",
    "href": "index.html#background-1",
    "title": "",
    "section": "Background",
    "text": "Background\nThere are two classes of models designed to handle data with excessive zeros: zero-inflated (ZI) models and hurdle models.\n\nIn ZI models, zeros can come from both binary and non-zero parts, whereas in hurdle models, zeros occur only in the binary part.\n\n\nOur approach is similar to hurdle models, but ours applies a soft threshold that replaces zeros with small non-zero values rather than generating exact zeros.\n\nBased on the continuous extension (CE) approach by Denuit and Lambert (2005).\n\n\n\nThe CE approach\n\nAssociates each discrete random variable (RV), \\(Y_i\\), with a continuous RV, \\(Y_i^*\\).\nHas only applied in discrete and spatial settings to date."
  },
  {
    "objectID": "index.html#proposed-method-1",
    "href": "index.html#proposed-method-1",
    "title": "",
    "section": "Proposed Method",
    "text": "Proposed Method\nTo construct zero-inflated Gamma for the marginal distribution,\n\nThe Gamma distribution is first reparametrized in terms of the mean, \\(\\mu\\), and the scale parameter, \\(\\beta\\).\n\n\n\nZero values are then replaced with non-zero values drawn from a uniform distribution.\n\nSpecifically,\n\\[\\begin{equation}\n0 \\leftarrow U_i, \\tag{5}\n\\end{equation}\\]\nwhere \\(U_i\\) follows a continuous uniform distribution on \\((0, \\epsilon)\\) with \\(\\epsilon\\) is a data-driven paramater representing the smallest observed non-zero values."
  },
  {
    "objectID": "index.html#proposed-method-2",
    "href": "index.html#proposed-method-2",
    "title": "",
    "section": "Proposed Method",
    "text": "Proposed Method\nThe resulting distribution, denoted as \\(ZIGamma(\\mu, \\beta, P, \\epsilon)\\), is expressed as:\n\\[\\begin{equation}\nf(x) =\n\\begin{cases}\nUnif(0, \\epsilon) & \\text{with probability } P \\\\\nShiftedGamma(\\mu, \\beta; \\epsilon) & \\text{with probability } 1-P,\n\\end{cases} \\tag{6}\n\\end{equation}\\]\nwhere \\(\\mu\\) denotes the mean and \\(\\beta\\) the scale parameter of the shifted Gamma distribution, \\(P \\in [0, 1]\\) the zero-inflated probability, and \\(\\epsilon &gt; 0\\) the threshold parameter.\n\nIn (6), there are two parts:\n\nWe have already discussed the uniform distribution.\nThe shifted Gamma distribution, \\(ShiftedGamma(\\mu, \\beta; \\epsilon)\\), is a standard Gamma distribution with mean \\(\\mu\\) and scale \\(\\beta\\) that is shifted to the right by \\(\\epsilon\\), with the support \\([\\epsilon, \\infty)\\)."
  },
  {
    "objectID": "index.html#dzig-pdf-of-zigamma",
    "href": "index.html#dzig-pdf-of-zigamma",
    "title": "",
    "section": "dzig (pdf of zigamma)",
    "text": "dzig (pdf of zigamma)\n\n\\(P = 0.1\\) \\(\\epsilon = 0.1\\)\\(P = 0.1\\) \\(\\epsilon = 0.4\\)\\(P = 0.5\\) \\(\\epsilon = 0.1\\)\\(P = 0.5\\) \\(\\epsilon = 0.4\\)\\(P = 0.7\\) \\(\\epsilon = 0.1\\)\\(P = 0.7\\) \\(\\epsilon = 0.4\\)"
  },
  {
    "objectID": "index.html#pzig-cdf-of-zigamma",
    "href": "index.html#pzig-cdf-of-zigamma",
    "title": "",
    "section": "pzig (cdf of zigamma)",
    "text": "pzig (cdf of zigamma)\n\n\\(P = 0.1\\) \\(\\epsilon = 0.1\\)\\(P = 0.1\\) \\(\\epsilon = 0.4\\)\\(P = 0.5\\) \\(\\epsilon = 0.1\\)\\(P = 0.5\\) \\(\\epsilon = 0.4\\)\\(P = 0.7\\) \\(\\epsilon = 0.1\\)\\(P = 0.7\\) \\(\\epsilon = 0.4\\)"
  },
  {
    "objectID": "index.html#simulation-setting",
    "href": "index.html#simulation-setting",
    "title": "",
    "section": "Simulation Setting",
    "text": "Simulation Setting\nSimulated data:\n\nWith weight \\(\\boldsymbol{w}\\), dependence for Gaussian copula \\(\\boldsymbol{\\rho}\\), mean \\(\\mu\\), scale \\(\\beta\\), zero-inflated probability \\(P\\), and threshold parameter \\(\\epsilon\\), we generate \\(n = 2000\\) observations from the copula-based ZIGamma MTD model.\n\nModel fitting:\n\nSet the order \\(L = 5\\) and consider the Gaussian copula with Gamma marginals.\n\nMCMC setting:\n\nRun the Gibbs sampler for \\(165, 000\\) iterations, discard the first \\(5000\\) iterations as burn-in, and collect samples every \\(20\\) iterations.\nRun four MCMC chains with \\(8000\\) iterations each for all of the following scenarios."
  },
  {
    "objectID": "index.html#simulation-studies-1",
    "href": "index.html#simulation-studies-1",
    "title": "",
    "section": "Simulation Studies",
    "text": "Simulation Studies\nWe have assessed accuracy and performance of the proposed model.\n\nConvergence Diagnostics (\\(\\boldsymbol{w}\\), \\(\\boldsymbol{\\rho}\\), \\(\\mu\\), \\(\\beta\\), \\(P\\), \\(\\epsilon\\))\nWeight and Dependence Parameters for Copula (\\(\\boldsymbol{w}\\), \\(\\boldsymbol{\\rho}\\))\nParameters for Marginal Distribution (\\(\\mu\\), \\(\\beta\\), \\(P\\), \\(\\epsilon\\))\n\nSimulation studies demonstrate promising results across various scenarios.\n\nScenario 1 and 2: Default setup; Compatible \\(\\boldsymbol{w}\\) and \\(\\boldsymbol{\\rho}\\)\nScenario 3-6: Usual case for zero-inflated gamma marginals\nScenario 7-9: Unusual case involving high skewness"
  },
  {
    "objectID": "index.html#simulation-studies-2",
    "href": "index.html#simulation-studies-2",
    "title": "",
    "section": "Simulation Studies",
    "text": "Simulation Studies\nIn all nine scenarios, we vary \\(P\\) and \\(\\epsilon\\), resulting in six cases per scenario:\n\n\\(P = 0.1\\), \\(\\epsilon = 0.1\\)\n\\(P = 0.1\\), \\(\\epsilon = 0.4\\)\n\\(P = 0.5\\), \\(\\epsilon = 0.1\\)\n\\(P = 0.5\\), \\(\\epsilon = 0.4\\)\n\\(P = 0.7\\), \\(\\epsilon = 0.1\\)\n\\(P = 0.7\\), \\(\\epsilon = 0.4\\)\n\nFor brevity, we present Scenario 1 only."
  },
  {
    "objectID": "index.html#simulation-results-res",
    "href": "index.html#simulation-results-res",
    "title": "",
    "section": "Simulation Results 1",
    "text": "Simulation Results 1\n\n\\(P = 0.1\\) \\(\\epsilon = 0.1\\)\\(P = 0.1\\) \\(\\epsilon = 0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\nMean (SD)\nR (Upper CI)\nNaive SE\nTime-series SE\n\n\n\n\n\\(w_1 = 0.636\\)\n0.6395 (0.0425)\n1 (1)\n2e-04\n0.0003\n\n\n\\(w_2 = 0.234\\)\n0.1905 (0.0636)\n1.01 (1.01)\n4e-04\n0.0013\n\n\n\\(w_3 = 0.086\\)\n0.1315 (0.0739)\n1 (1)\n4e-04\n0.0021\n\n\n\\(w_4 = 0.032\\)\n0.0346 (0.0529)\n1.01 (1.03)\n3e-04\n0.0017\n\n\n\\(w_5 = 0.012\\)\n0.0039 (0.0171)\n1 (1)\n1e-04\n0.0004\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\nMean (SD)\nR (Upper CI)\nNaive SE\nTime-series SE\n\n\n\n\n\\(\\rho_1 = 0.700\\)\n0.6847 (0.0274)\n1 (1)\n0.0002\n0.0002\n\n\n\\(\\rho_2 = 0.500\\)\n0.606 (0.1426)\n1.01 (1.01)\n0.0008\n0.0027\n\n\n\\(\\rho_3 = 0.300\\)\n0.1168 (0.2389)\n1 (1)\n0.0013\n0.0018\n\n\n\\(\\rho_4 = 0.100\\)\n0.0147 (0.4675)\n1 (1)\n0.0026\n0.0027\n\n\n\\(\\rho_5 = 0.100\\)\n-0.0046 (0.5659)\n1 (1)\n0.0032\n0.0032\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{w}\\), \\(\\boldsymbol{\\rho}\\) for all other combinations of \\(P = 0.1, 0.5, 0.7\\) and \\(\\epsilon = 0.1, 0.4\\) are similar and therefore omitted."
  },
  {
    "objectID": "index.html#simulation-results",
    "href": "index.html#simulation-results",
    "title": "",
    "section": "Simulation Results",
    "text": "Simulation Results\n\n\\(P = 0.1\\) \\(\\epsilon = 0.1\\)\\(P = 0.1\\) \\(\\epsilon = 0.4\\)\\(P = 0.5\\) \\(\\epsilon = 0.1\\)\\(P = 0.5\\) \\(\\epsilon = 0.4\\)\\(P = 0.7\\) \\(\\epsilon = 0.1\\)\\(P = 0.7\\) \\(\\epsilon = 0.4\\)\n\n\n\n\n\n\n\n.\nMean (SD)\nR (Upper CI)\nNaive SE\nTime-series SE\n\n\n\n\n\\(\\mu\\)\n7.35 (0.1132)\n1 (1)\n6e-04\n6e-04\n\n\n\\(\\beta\\)\n1.0082 (0.0433)\n1 (1)\n2e-04\n2e-04\n\n\n\\(P\\)\n0.0769 (0.0085)\n1 (1)\n0e+00\n0e+00\n\n\n\\(\\epsilon\\)\n0.1 (7e-04)\n1 (1)\n0e+00\n0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n.\nMean (SD)\nR (Upper CI)\nNaive SE\nTime-series SE\n\n\n\n\n\\(\\mu\\)\n7.1454 (0.12)\n1 (1)\n7e-04\n7e-04\n\n\n\\(\\beta\\)\n1.0994 (0.0472)\n1 (1)\n3e-04\n3e-04\n\n\n\\(P\\)\n0.1091 (0.0103)\n1 (1)\n1e-04\n1e-04\n\n\n\\(\\epsilon\\)\n0.4017 (0.0019)\n1 (1)\n0e+00\n0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n.\nMean (SD)\nR (Upper CI)\nNaive SE\nTime-series SE\n\n\n\n\n\\(\\mu\\)\n6.9447 (0.1207)\n1 (1)\n7e-04\n7e-04\n\n\n\\(\\beta\\)\n1.0659 (0.0542)\n1 (1)\n3e-04\n3e-04\n\n\n\\(P\\)\n0.5248 (0.0172)\n1 (1)\n1e-04\n1e-04\n\n\n\\(\\epsilon\\)\n0.1001 (1e-04)\n1 (1)\n0e+00\n0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n.\nMean (SD)\nR (Upper CI)\nNaive SE\nTime-series SE\n\n\n\n\n\\(\\mu\\)\n6.8454 (0.1154)\n1 (1)\n6e-04\n6e-04\n\n\n\\(\\beta\\)\n1.0086 (0.0512)\n1 (1)\n3e-04\n3e-04\n\n\n\\(P\\)\n0.5064 (0.0173)\n1 (1)\n1e-04\n1e-04\n\n\n\\(\\epsilon\\)\n0.4 (4e-04)\n1 (1)\n0e+00\n0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n.\nMean (SD)\nR (Upper CI)\nNaive SE\nTime-series SE\n\n\n\n\n\\(\\mu\\)\n6.988 (0.1303)\n1 (1)\n7e-04\n7e-04\n\n\n\\(\\beta\\)\n0.9593 (0.0594)\n1 (1)\n3e-04\n3e-04\n\n\n\\(P\\)\n0.6879 (0.016)\n1 (1)\n1e-04\n1e-04\n\n\n\\(\\epsilon\\)\n0.0999 (1e-04)\n1 (1)\n0e+00\n0e+00\n\n\n\n\n\n\n\n\n\n\n\n\n.\nMean (SD)\nR (Upper CI)\nNaive SE\nTime-series SE\n\n\n\n\n\\(\\mu\\)\n6.8482 (0.1373)\n1 (1)\n8e-04\n8e-04\n\n\n\\(\\beta\\)\n1.0506 (0.0665)\n1 (1)\n4e-04\n4e-04\n\n\n\\(P\\)\n0.7048 (0.0154)\n1 (1)\n1e-04\n1e-04\n\n\n\\(\\epsilon\\)\n0.4002 (3e-04)\n1 (1)\n0e+00\n0e+00"
  },
  {
    "objectID": "index.html#coverage-assessment",
    "href": "index.html#coverage-assessment",
    "title": "",
    "section": "Coverage Assessment",
    "text": "Coverage Assessment\nAll nine scenarios were analyzed using a single replicate.\n\nRecall that we have six cases per scenario.\n\n\nScenarios 1 and 2 were further evaluated with \\(10\\) replicates to assess coverage and robustness.\n\nThe overall coverage is the proportion of replicates for which the true value is contained within the interval.\n\n\n\nMost parameters achieve full coverage, with a few slightly below \\(1\\), indicating that the posterior intervals reliably capture the true parameter values."
  },
  {
    "objectID": "index.html#prediction-95-predictive-intervals-one-step-ahead",
    "href": "index.html#prediction-95-predictive-intervals-one-step-ahead",
    "title": "",
    "section": "Prediction, \\(95\\%\\) Predictive Intervals, One-Step Ahead",
    "text": "Prediction, \\(95\\%\\) Predictive Intervals, One-Step Ahead\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoverage\nBelow\nAbove\n\n\n\n\nP01Eps01\n0.9549\n0.6944\n0.9751\n\n\nP01Eps04\n0.9148\n0.3973\n0.9786\n\n\nP05Eps01\n0.9278\n0.9876\n0.8615\n\n\nP05Eps04\n0.9484\n0.9681\n0.9285\n\n\nP07Eps01\n0.8677\n0.9943\n0.5726\n\n\nP07Eps04\n0.9298\n0.9888\n0.7828\n\n\n\n\n\n\n\n\nWhen the zero-inflated probability is low (e.g., \\(P = 0.1\\)), the empirical coverage above (i.e., the coverage for data greater than \\(\\epsilon\\)) is a more informative metric for assessing predictive performance.\nAs \\(P\\) increases (e.g., \\(P = 0.5\\), \\(0.7\\)), the empirical coverage below becomes increasingly dominant.\nThe model appropriately captures the predictive uncertainty across all cases."
  },
  {
    "objectID": "index.html#prediction-95-predictive-intervals-one-step-ahead-1",
    "href": "index.html#prediction-95-predictive-intervals-one-step-ahead-1",
    "title": "",
    "section": "Prediction, \\(95\\%\\) Predictive Intervals, One-Step Ahead",
    "text": "Prediction, \\(95\\%\\) Predictive Intervals, One-Step Ahead\n\n\\(P = 0.1\\) \\(\\epsilon = 0.1\\)\\(P = 0.1\\) \\(\\epsilon = 0.4\\)\\(P = 0.5\\) \\(\\epsilon = 0.1\\)\\(P = 0.5\\) \\(\\epsilon = 0.4\\)\\(P = 0.7\\) \\(\\epsilon = 0.1\\)\\(P = 0.7\\) \\(\\epsilon = 0.4\\)"
  },
  {
    "objectID": "index.html#introduction-2",
    "href": "index.html#introduction-2",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nRecurrent Neural Networks (RNNs), and their variants, Long Short-Term Memory (LSTMs), are widely used for modeling sequence data because of their ability to capture both short- and long-term dependencies.\n\nBeyond natural language processing (NLP), RNNs and LSTMs have also shown effective in time series forecasting and have been employed for applications including\n\nfinancial market prediction,\nenergy forecasting,\nweather and climate modeling, and\nepidemiological trend analysis."
  },
  {
    "objectID": "index.html#introduction-and-objectives-2",
    "href": "index.html#introduction-and-objectives-2",
    "title": "",
    "section": "Introduction and Objectives",
    "text": "Introduction and Objectives\nHowever,\n\nprevious studies comparing LSTMs to traditional models such as ARIMA often claim LSTM superiority, a conclusion that can be misleading when the benchmarks chosen are inappropriate.\nA prior study found comparable performance between the MTD model and the LSTM network for predicting disease spread.\n\n\n\nObjectives: Evaluate LSTM and MTD models to ensure a fair and balanced comparison of their performance and robustness.\n\n\n\n\nDemonstrate that our proposed MTDs outperform LSTMs in accuracy, though at a higher computational cost and the need for careful design."
  },
  {
    "objectID": "index.html#rnn-recurrent-neural-network",
    "href": "index.html#rnn-recurrent-neural-network",
    "title": "",
    "section": "RNN (Recurrent Neural Network)",
    "text": "RNN (Recurrent Neural Network)\n\n\n\n\n\n\nArchitecture of an RNN unit, reproduced from Olah (2015). \\(x_t\\) is the input, \\(h_t\\) is the hidden state, and \\(o_t\\) is the output. \\(tanh\\) is the activation function, squashing values to \\((-1, 1)\\) for stability and zero-centered output.\n\n\n\n\n\nAn RNN unit computes a weighted combination of input data, \\(x_t\\), and the previous hidden state, \\(h_{t-1}\\), applies an activation function, and updates the hidden state to \\(h_t\\).\nRNN is composed of repeating units that unfold over time, where each unit passes recurrent information stored in the hidden state from one time step to the next.\nRNN is prone to the well-documented vanishing gradient issue when processing long sequences."
  },
  {
    "objectID": "index.html#lstm-long-short-term-memory",
    "href": "index.html#lstm-long-short-term-memory",
    "title": "",
    "section": "LSTM (Long Short-Term Memory)",
    "text": "LSTM (Long Short-Term Memory)\n\n\n\n\n\n\nArchitecture of an LSTM unit with a forget gate, reproduced from Olah (2015). \\(x_t\\) is the input, \\(h_t\\) the hidden state, and \\(c_t\\) the cell state. \\(f_t\\), \\(i_t\\), and \\(o_t\\) are the forget, input, and output gates, respectively. \\(\\sigma\\) is used to squash values to \\((0, 1)\\) for gating, while \\(tanh\\) squashes values to \\((-1, 1)\\) for stability and zero-centered output.\n\n\n\n\n\nLSTM extends RNN by introducing a cell state and three gates: the forget gate, the input gate, and the output gate.\nThe cell state carries long-term dependence, while the hidden state encodes short-term patterns.\nThe gates control information flow, deciding what to forget, add, and pass to the hidden state at each step."
  },
  {
    "objectID": "index.html#hyperparameter-tuning-and-training",
    "href": "index.html#hyperparameter-tuning-and-training",
    "title": "",
    "section": "Hyperparameter Tuning and Training",
    "text": "Hyperparameter Tuning and Training\nHyperparameter tuning plays a crucial role in improving model performance. Key hyperparameters include:\n\nbatch size,\nnumber of epochs,\nlearning rate\nnumber of hidden units or cell dimension,\nnumber of hidden layers, etc.\n\n\nThe network is typically trained using Backpropagation Through Time (BPTT) and optimized with gradient-based methods such as Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam)."
  },
  {
    "objectID": "index.html#experimental-setup",
    "href": "index.html#experimental-setup",
    "title": "",
    "section": "Experimental Setup",
    "text": "Experimental Setup\nExperiment 1:\n\nWe run both models on Gamma Scenario 1‚Äì9 to compare the predictive performance of the LSTM and MTD models under various conditions.\n\nExperiment 2:\n\nWe run both models on 10 independently generated replicates of Gamma Scenario 1 to assess the stability and robustness of model performance.\n\nExperiment 3:\n\nWe run the LSTM model with a variety of configurations on Gamma Scenario 1 with 10 independently generated replicates for each configuration to investigate the impact of hyperparameters."
  },
  {
    "objectID": "index.html#experimental-setup-1",
    "href": "index.html#experimental-setup-1",
    "title": "",
    "section": "Experimental Setup",
    "text": "Experimental Setup\nExperiment 4:\n\nFor the zero-inflated Gamma settings, we run both models on Scenario 1 and 2, since each scenario includes six cases defined by all combinations of \\(P = 0.1, 0.5, 0.7\\), and \\(\\epsilon = 0.1, 0.4\\)."
  },
  {
    "objectID": "index.html#simulation-results-for-gamma-scenarios-1-9",
    "href": "index.html#simulation-results-for-gamma-scenarios-1-9",
    "title": "",
    "section": "Simulation Results for Gamma Scenarios 1-9",
    "text": "Simulation Results for Gamma Scenarios 1-9\n\n\n\n\n\n\n\n\n\n\n\n\n.\nLSTM\nMTD\n\n\n\n\ns1\n1.3326\n1.3569\n\n\ns2\n2.3001\n2.1988\n\n\ns3\n1.07\n1.0446\n\n\ns4\n1.6846\n1.5282\n\n\ns5\n1.0215\n1.1296\n\n\ns6\n0.8263\n0.7649\n\n\ns7\n0.7452\n0.7617\n\n\ns8\n0.3675\n0.3808\n\n\ns9\n0.1837\n0.1902\n\n\n\n\n\n\n\nRMSEs for MTD are lower in Scenarios 2, 3, 4, and 6, though the differences are minimal.\nRMSEs are the highest for both models in Scenario 2."
  },
  {
    "objectID": "index.html#simulation-results-for-gamma-scenario-1-10-replicates",
    "href": "index.html#simulation-results-for-gamma-scenario-1-10-replicates",
    "title": "",
    "section": "Simulation Results for Gamma Scenario 1, 10 replicates",
    "text": "Simulation Results for Gamma Scenario 1, 10 replicates\n\n\n\n\n\n\n\n\n\n\n\n\n\nRMSE has a mean difference of \\(0.128957\\) (p-value = 0.005175, df = 9), with MTD consistently yielding lower RMSEs."
  },
  {
    "objectID": "index.html#simulation-results-for-gamma-scenario-1-10-replicates-1",
    "href": "index.html#simulation-results-for-gamma-scenario-1-10-replicates-1",
    "title": "",
    "section": "Simulation Results for Gamma Scenario 1, 10 replicates",
    "text": "Simulation Results for Gamma Scenario 1, 10 replicates\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatch size is significant (Pr(&gt;F) = 4.4e-06, df = 2, 24); RMSEs differ only for \\(64\\) vs.¬†others, and \\(128\\) vs.¬†others.\nCell dimensions too (Pr(&gt;F) = 0.0113, df = 2, 24); RMSE differs only between \\(32\\) vs.¬†\\(64\\) and \\(32\\) vs.¬†\\(128\\).\nWe adopt the default configuration for subsequent experiments:\n\nlearning rate = \\(0.001\\),\nbatch size = \\(32\\),\nnumber of layers = \\(1\\), and\ncell dimension = \\(64\\)."
  },
  {
    "objectID": "index.html#simulation-results-for-zigamma-scenario-1",
    "href": "index.html#simulation-results-for-zigamma-scenario-1",
    "title": "",
    "section": "Simulation Results for ZIGamma Scenario 1",
    "text": "Simulation Results for ZIGamma Scenario 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\nLSTM\nMTD\n\n\n\n\nP01Eps01\n1.7221\n1.7302\n\n\nP01Eps04\n2.0843\n2.6386\n\n\nP05Eps01\n2.1102\n2.8361\n\n\nP05Eps04\n2.1788\n2.1922\n\n\nP07Eps01\n2.2762\n3.0739\n\n\nP07Eps04\n2.0916\n2.5798\n\n\n\n\n\n\n\n\nLSTM generally achieves lower overall RMSEs compared to MTD.\n‚ùóÔ∏èBut, patterns similar to those in Prediction of Part II reappear."
  },
  {
    "objectID": "index.html#simulation-results-for-zigamma-scenario-1-1",
    "href": "index.html#simulation-results-for-zigamma-scenario-1-1",
    "title": "",
    "section": "Simulation Results for ZIGamma Scenario 1",
    "text": "Simulation Results for ZIGamma Scenario 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\nLSTM Below\nMTD Below\nLSTM Above\nMTD Above\n\n\n\n\nP01Eps01\n3.6285\n4.2964\n1.4910\n1.3668\n\n\nP01Eps04\n3.2122\n5.9082\n1.8826\n1.7945\n\n\nP05Eps01\n2.0551\n0.7752\n2.1655\n3.9641\n\n\nP05Eps04\n2.0495\n1.6675\n2.3426\n2.7477\n\n\nP07Eps01\n1.2984\n0.3677\n3.5798\n5.458\n\n\nP07Eps04\n1.2583\n0.5346\n3.3044\n4.6465\n\n\n\n\n\n\n\n\nWhen the zero-inflated probability is low (e.g., \\(P = 0.1\\)), the RMSE above (which reflects predictive accuracy for values exceeding \\(\\epsilon\\)) is a more informative metric for assessing predictive performance.\nAs \\(P\\) increases (e.g., \\(P = 0.5\\), \\(0.7\\)), this relationship reverses, and the RMSE below becomes more relevant.\nMTD outperforms LSTM in RMSE above for \\(P = 0.1\\) and again yields lower values for RMSE below than LSTM at higher levels of zero-inflation (e.g., \\(P = 0.5\\), \\(0.7\\))."
  },
  {
    "objectID": "index.html#experimental-setup-2",
    "href": "index.html#experimental-setup-2",
    "title": "",
    "section": "Experimental Setup",
    "text": "Experimental Setup\n\n\nMTD\n\nFor MTD, the hyperparameter settings are:\n\nmarginal parameters \\(\\alpha\\), \\(\\beta\\) 1: \\(Gamma(\\alpha | u_{\\alpha}, v_{\\alpha} = 1)\\), \\(Gamma(\\beta | u_{\\beta}, v_{\\beta} = 1)\\),\ndependence parameter \\(\\rho\\): \\(Unif(\\rho_l | -1, 1)\\), and\nweight parameter \\(w\\) 2: \\(CDP(w | \\alpha_0 = 5, a_0 = 1, b_0 = 3)\\); cdf-based Dirichlet process prior.\n\n\n\nMTD is implemented in R and executed on a high-performance computing (HPC) cluster.\n\n\nLSTM\n\nFor LSTM, we reuse the default configuration for subsequent experiments:\n\nlearning rate = \\(0.001\\),\nbatch size = \\(32\\),\nnumber of layers = \\(1\\), and\ncell dimension = \\(64\\).\n\nLSTM is implemented in PyTorch and trained on a standard workstation.\n\n\n\\(u_{\\alpha}\\), \\(u_{\\beta}\\) derived from the training data: \\(u_{\\beta}\\) = mean/variance, \\(u_{\\alpha}\\) = mean \\(\\times\\) \\(u_{\\beta}\\).\\(\\alpha_0\\): concentration parameter; \\(G_0\\): base distribution, i.e., \\(G_0 \\sim Beta(a_0, b_0)\\)."
  },
  {
    "objectID": "index.html#evaluation-metrics-metrics",
    "href": "index.html#evaluation-metrics-metrics",
    "title": "",
    "section": "Evaluation Metrics 1",
    "text": "Evaluation Metrics 1\n\n\n\n\n\n\n\n\nMetric\nDefinition\nFormula\n\n\n\n\nRMSE\nRoot Mean Squared Error\n\\(\\sqrt{\\frac{1}{T} \\sum_{t=1}^{T} (y_t - \\hat{y}_t)^2}\\)\n\n\nMAE\nMean Absolute Error\n\\(\\frac{1}{T} \\sum_{t=1}^{T} |y_t - \\hat{y}_t|\\)\n\n\nMAPE\nMean Absolute Percentage Error\n\\(\\frac{100}{T} \\sum_{t=1}^{T} \\left| \\frac{y_t - \\hat{y}_t}{y_t} \\right|\\)\n\n\nSMAPE\nSymmetric MAPE\n\\(\\frac{100}{T} \\sum_{t=1}^{T} \\frac{|y_t - \\hat{y}_t|}{(|y_t| + |\\hat{y}_t|) / 2}\\)\n\n\nMASE\nMean Absolute Scaled Error\n\\(\\frac{ \\frac{1}{T} \\sum_{t=1}^{T} |y_t - \\hat{y}_t| }{ \\frac{1}{T - 1} \\sum_{t=2}^{T} |y_t - y_{t-1}| }\\)\n\n\n\nMAPE, SMAPE, and MASE are scale-independent, allowing for comparison across datasets with different units."
  },
  {
    "objectID": "index.html#data-application-predicted-means-one-step-ahead-test-size-n-1756-caption",
    "href": "index.html#data-application-predicted-means-one-step-ahead-test-size-n-1756-caption",
    "title": "",
    "section": "Data Application, Predicted Means, One-Step ahead; Test Size \\(n = 1756\\) 1",
    "text": "Data Application, Predicted Means, One-Step ahead; Test Size \\(n = 1756\\) 1\n\nwindspeeds at 50m above groundwindspeeds at 10m above groundwindspeeds at 2m above ground\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means."
  },
  {
    "objectID": "index.html#data-application-predicted-means-one-step-ahead-zoom-in-view-n-200-caption",
    "href": "index.html#data-application-predicted-means-one-step-ahead-zoom-in-view-n-200-caption",
    "title": "",
    "section": "Data Application, Predicted Means, One-Step Ahead; Zoom-In View \\(n = 200\\) 1",
    "text": "Data Application, Predicted Means, One-Step Ahead; Zoom-In View \\(n = 200\\) 1\n\nwindspeeds at 50m above groundwindspeeds at 10m above groundwindspeeds at 2m above ground\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means."
  },
  {
    "objectID": "index.html#data-application-prediction-errors-one-step-ahead-zoom-in-view-n-200-captiondiff",
    "href": "index.html#data-application-prediction-errors-one-step-ahead-zoom-in-view-n-200-captiondiff",
    "title": "",
    "section": "Data Application, Prediction Errors, One-Step Ahead; Zoom-In View \\(n = 200\\) 1",
    "text": "Data Application, Prediction Errors, One-Step Ahead; Zoom-In View \\(n = 200\\) 1\n\nwindspeeds at 50m above groundwindspeeds at 10m above groundwindspeeds at 2m above ground\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDashed (red) lines show differences between LSTM predicted means and observed values and dashed (blue) lines show differences between MTD predicted means and observed values."
  },
  {
    "objectID": "index.html#data-application-95-predictive-intervals-one-step-ahead-mtd-only-captionmtd",
    "href": "index.html#data-application-95-predictive-intervals-one-step-ahead-mtd-only-captionmtd",
    "title": "",
    "section": "Data Application, \\(95\\%\\) Predictive Intervals, One-Step Ahead; MTD Only 1",
    "text": "Data Application, \\(95\\%\\) Predictive Intervals, One-Step Ahead; MTD Only 1\n\nwindspeeds at 50m above groundwindspeeds at 10m above groundwindspeeds at 2m above ground\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmpirical coverage is particularly relevant and can only be assessed with probabilistic forecasting methods such as MTD."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nPart I and II\nWe develop a novel copula-based MTD model that separates the dependence structure from the marginal distribution. This separation\n\nEnables a choice of copula families that effectively capture dependence.\nProvides flexibility in marginal selection.\n\nWe also develop a copula-based zero-inflated MTD model, which preserves the advantages of copula modeling for capturing dependence in zero-inflated continuous settings.\n\nThis extension again demonstrates the framework‚Äôs generalizability beyond the Gamma marginals."
  },
  {
    "objectID": "index.html#conclusion-1",
    "href": "index.html#conclusion-1",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nPart III\nThrough simulation studies and real-world applications, we illustrate how MTD and LSTM serve as complementary approaches for modeling complex, skewed, and zero-inflated time series.\n\nMTD models provide more robust and explainable temporal modeling but come at the cost of increased computational complexity and more involved model design.\nLSTMs offer general-purpose modeling and computational efficiency, but their black-box nature can limit interpretability and, in some cases, reduce accuracy.\n\nOverall, the proposed MTD framework provides a flexible, robust, and interpretable approach for modeling complex continuous and zero-inflated time series, demonstrating superior performance compared to LSTM in the settings considered."
  },
  {
    "objectID": "index.html#future-work",
    "href": "index.html#future-work",
    "title": "",
    "section": "Future Work",
    "text": "Future Work\nPart I and II\n\nBoth Gamma MTD and ZIGamma MTD models use Gaussian copula to model dependence. Future work should explore alternative copulas, like Clayton and Gumbel, to capture tail dependence and asymmetry, and assess their impact on model performance.\nAlthough the current framework assumes stationarity and does not incorporate covariates/features, it can be easily extended to non-stationary time series and models with covariates/features, further enhancing flexibility and realism in modeling complex temporal patterns."
  },
  {
    "objectID": "index.html#future-work-1",
    "href": "index.html#future-work-1",
    "title": "",
    "section": "Future Work",
    "text": "Future Work\nPart III\n\nGiven the growing prominence of transformer architectures in sequence modeling, further research should extend performance comparisons to include transformer-based models.\nIt is equally important to ensure that evaluations are conducted using appropriate benchmarks and metrics."
  },
  {
    "objectID": "index.html#acknowledgement",
    "href": "index.html#acknowledgement",
    "title": "",
    "section": "Acknowledgement",
    "text": "Acknowledgement"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "index.html#copula-cdf-and-pdf",
    "href": "index.html#copula-cdf-and-pdf",
    "title": "",
    "section": "Copula CDF and PDF",
    "text": "Copula CDF and PDF\n\n\nWireframe plot of bivariate Gaussian distribution and density function, reproduced from Kenton (2024). Retrieved from https://www.investopedia.com/terms/c/copula.asp."
  },
  {
    "objectID": "index.html#copula-families",
    "href": "index.html#copula-families",
    "title": "",
    "section": "Copula Families",
    "text": "Copula Families\n\n\nContour plot of different bivariate density function where marginals are \\(N(0, 1)\\), reproduced from Chang (2019). Retrieved from https://bochang.me/blog/posts/copula/."
  },
  {
    "objectID": "index.html#copula-sklars-theorem",
    "href": "index.html#copula-sklars-theorem",
    "title": "",
    "section": "Copula: Sklar‚Äôs theorem",
    "text": "Copula: Sklar‚Äôs theorem\n\nFor any \\(p\\)-dimensional multivariate cumulative distribution function (cdf) of a random vector \\((X_1, ..., X_p)\\), denoted as \\(F(x_1, ..., x_p)\\), there exists a copula function \\(C: {[0,1]}^p \\rightarrow [0, 1]\\) for which \\(F(x_1, ..., x_p) = C(F_1(x_1), ..., F_p(x_p))\\), where \\(F_j\\) is the marginal cdf of \\(X_j, j = 1,..., p\\). If \\(X_j\\) is continuous for all \\(j\\), then \\(C\\) is unique and differentiable. The joint probability density function (pdf) of \\(X_j, j = 1,...,p\\) is given by \\(f(x_j) = c(x_j) \\prod_{j=1}^p f_j(x_j)\\), where \\(c = \\partial^p{C} / \\partial{F_j}\\) is the copula density and \\(f_j\\) is the density of \\(X_j\\)."
  },
  {
    "objectID": "index.html#proposition-1-zheng-et-al.-2022s-invariant-condition",
    "href": "index.html#proposition-1-zheng-et-al.-2022s-invariant-condition",
    "title": "",
    "section": "Proposition 1: Zheng et al.¬†(2022)‚Äôs invariant condition",
    "text": "Proposition 1: Zheng et al.¬†(2022)‚Äôs invariant condition\n\nConsider a set of bivariate random vectors \\((U_l, V_l)\\) that takes values in \\(S \\times S \\subset \\mathbb{R}\\). Consider a time series \\(\\{ X_t: t \\in \\mathbb{N} \\}\\), where \\(X_t \\in S\\), generated from \\[\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^L w_l f_{U_l|V_l} (x_t | x_{t-l}), t &gt; L,\n\\] and from \\[\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^{t-2} w_l f_{U_l|V_l} (x_t | x_{t-l}) +\n\\left( 1 - \\sum_{k=1}^{t-2} w_k \\right) f_{U_{t-1} | V_{t-1}} (x_t|x_1), 2 \\leq t \\leq L.\n\\]\nIf a time series satisfies the invariant condition: \\(X_1 \\sim f_X\\), and \\(f_{U_l}(x) = f_{V_l}(x) = f_X(x)\\), for all \\(x \\in S\\), and for all \\(l\\), then this time time series is first-order strictly stationary with invariant marginal density \\(f_X\\)."
  },
  {
    "objectID": "index.html#algorithm-1-implementaion-details",
    "href": "index.html#algorithm-1-implementaion-details",
    "title": "",
    "section": "Algorithm 1: Implementaion Details",
    "text": "Algorithm 1: Implementaion Details\n\nThe full Bayesian model is complete by priors specification for the parameters \\(\\boldsymbol{\\theta} = \\{\\alpha, \\beta, \\boldsymbol{\\rho}\\}\\) and \\(\\boldsymbol{w}\\).\nThe posterior full conditional distributions:\n\nFor the marginal parameters \\(\\alpha\\) and \\(\\beta\\) are proportional to \\(Gamma(\\alpha | u_{\\alpha}, v_{\\alpha}) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\) and \\(Gamma(\\beta | u_{\\beta}, v_{\\beta}) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\), respectively. \\(f_l (x_t | x_{t-l}) = c_l (x_t, x_{t-l}) f_X(x_t)\\).\nFor each of the dependence parameters \\(\\boldsymbol{\\rho}\\) is proportional to \\(Unif(\\rho_l |-1, 1) \\prod_{t:z_t = l} c_l (x_t, x_{t-l})\\).\nFor each \\(z_t\\) of the latent variables \\({\\{z_t\\}}_{t=L+1}^n\\), where the probability of \\(z_t = l\\), denoted by \\(q_l\\), is proportional to \\(w_l c_l (x_t, x_{t-l})\\), for \\(l = 1,..., L\\).\nFor weight parameters \\(\\boldsymbol{w}\\), under the cdf-based prior, is \\(Dirichlet (\\boldsymbol{\\alpha})\\), where \\(\\boldsymbol{\\alpha} = (\\alpha_0 a_1 + M_1, ..., \\alpha_0 a_L + M_L)\\). \\(a_l = G_0(l/L) - G_0((l - 1)/L)\\), \\(M_l = |\\{ t: z_t = l \\}|\\)."
  },
  {
    "objectID": "index.html#algorithm-2-implementaion-details",
    "href": "index.html#algorithm-2-implementaion-details",
    "title": "",
    "section": "Algorithm 2: Implementaion Details",
    "text": "Algorithm 2: Implementaion Details\n\nThe full Bayesian model is complete by priors specification for the parameters \\(\\boldsymbol{\\theta} = \\{\\alpha, \\beta, P, \\epsilon, \\boldsymbol{\\rho}\\}\\) and \\(\\boldsymbol{w}\\).\nThe posterior full conditional distributions:\n\n(Same as Gamma MTD)\nFor the zero-inflated probability \\(P\\), is proportional to \\(Unif(P | 0, 1) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\).\nFor the threshold parameter \\(\\epsilon\\), is proportional to \\(ScaledBeta(5, 5; 0, 2\\epsilon_0) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\)."
  },
  {
    "objectID": "index.html#prior-analysis-for-gamma-scenario-1",
    "href": "index.html#prior-analysis-for-gamma-scenario-1",
    "title": "",
    "section": "Prior Analysis for Gamma Scenario 1",
    "text": "Prior Analysis for Gamma Scenario 1\n\n\\(\\alpha\\)\\(\\beta\\)"
  },
  {
    "objectID": "index.html#trace-and-density-plot-for-zigamma-scenario-1s-w",
    "href": "index.html#trace-and-density-plot-for-zigamma-scenario-1s-w",
    "title": "",
    "section": "Trace and Density Plot for ZIGamma Scenario 1‚Äôs \\(w\\)",
    "text": "Trace and Density Plot for ZIGamma Scenario 1‚Äôs \\(w\\)\n\n\\(w_1\\)\\(w_2\\)\\(w_3\\)\\(w_4\\)\\(w_5\\)"
  },
  {
    "objectID": "index.html#trace-and-density-plot-for-zigamma-scenario-1s-rho",
    "href": "index.html#trace-and-density-plot-for-zigamma-scenario-1s-rho",
    "title": "",
    "section": "Trace and Density Plot for ZIGamma Scenario 1‚Äôs \\(\\rho\\)",
    "text": "Trace and Density Plot for ZIGamma Scenario 1‚Äôs \\(\\rho\\)\n\n\\(\\rho_1\\)\\(\\rho_2\\)\\(\\rho_3\\)\\(\\rho_4\\)\\(\\rho_5\\)"
  },
  {
    "objectID": "index.html#trace-and-density-plot-for-zigamma-scenario-1s-mu-beta-p-epsilon",
    "href": "index.html#trace-and-density-plot-for-zigamma-scenario-1s-mu-beta-p-epsilon",
    "title": "",
    "section": "Trace and Density Plot for ZIGamma Scenario 1‚Äôs \\(\\mu, \\beta, P, \\epsilon\\)",
    "text": "Trace and Density Plot for ZIGamma Scenario 1‚Äôs \\(\\mu, \\beta, P, \\epsilon\\)\n\n\\(\\mu\\)\\(\\beta\\)\\(P\\)\\(\\epsilon\\)"
  },
  {
    "objectID": "index.html#rnn",
    "href": "index.html#rnn",
    "title": "",
    "section": "RNN",
    "text": "RNN\nLet \\(x_t\\), \\(h_t\\), and \\(o_t\\) denote the input data, the hidden state, and the output at time \\(t\\), respectively. Then, an RNN unit can be expressed as:\n\\[\\begin{equation}\n\\begin{split}\n\\label{eq:rnn}\nh_t &= f(W_{ih} h_{t-1} + W_{ix} x_t + b_i), \\\\\no_t &= g(W_{oh} \\cdot h_t + b_o),\n\\end{split}\n\\end{equation}\\]\nwhere \\(W_{ix}\\), \\(W_{ih}\\) and \\(W_{oh}\\) denote the weight matrices, and \\(b_i\\) and \\(b_o\\) the bias vectors.\nThe subscripts \\(i\\) and \\(o\\) indicate their steps in RNN: \\(i\\) refer to the input/hidden step, and \\(o\\) to the output step.\n\\(f\\) and \\(g\\) denote the activation functions for the hidden layer and output layer, respectively.\n\n\\(f\\) is typically set to the logistic sigmoid function, denoted as \\(\\sigma\\), which outputs values in range \\((0, 1)\\) to act as a gate that controls how much information passes through.\n\\(g\\) is the hyperbolic tangent function, denoted as \\(tanh\\), which outputs values in range \\((-1, 1)\\) to generate output in a stable, zero-centered range."
  },
  {
    "objectID": "index.html#lstm-1",
    "href": "index.html#lstm-1",
    "title": "",
    "section": "LSTM",
    "text": "LSTM\nLet \\(c_t\\) and \\(h_t\\) denote the cell and the hidden state vector. Let \\(f_t\\), \\(i_t\\), and \\(o_t\\) represent the forget, the input, and the output gate vector at time \\(t\\), respectively. Then, an LSTM unit can be expressed as:\n\\[\\begin{equation}\n\\begin{split}\n\\label{eq:lstm}\nf_t &= \\sigma(W_{fh} h_{t-1} + W_{fx} x_t + b_f), \\\\\ni_t &= \\sigma(W_{ih} h_{t-1} + W_{ix} x_t + b_i), \\\\\n\\tilde{c}_t &= \\tanh(W_{\\tilde{c} h} h_{t-1} + W_{\\tilde{c} x} x_t + b_{\\tilde{c}}), \\\\\nc_t &= f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t, \\\\\no_t &= \\sigma(W_{oh} h_{t-1} + W_{ox} x_t + b_o), \\\\\nh_t &= o_t \\cdot \\tanh(c_t),\n\\end{split}\n\\end{equation}\\]\nwhere \\(\\boldsymbol{W}\\) denotes the weight matrices, \\(\\boldsymbol{b}\\) the bias vectors, \\(\\sigma\\) the logistic sigmoid function, and \\(tanh\\) the hyperbolic tangent function."
  },
  {
    "objectID": "index.html#training-and-validation-loss-curves-for-lstm",
    "href": "index.html#training-and-validation-loss-curves-for-lstm",
    "title": "",
    "section": "Training and Validation Loss Curves for LSTM",
    "text": "Training and Validation Loss Curves for LSTM"
  },
  {
    "objectID": "index.html#training-and-validation-loss-curves-for-lstm-1",
    "href": "index.html#training-and-validation-loss-curves-for-lstm-1",
    "title": "",
    "section": "Training and Validation Loss Curves for LSTM",
    "text": "Training and Validation Loss Curves for LSTM"
  }
]